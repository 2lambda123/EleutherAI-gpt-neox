{"launcher": "jsrun", "train_batch_size": 276, "train_micro_batch_size_per_gpu": 138, "optimizer": {"type": "Adam", "params": {"lr": 0.00012, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "hysteresis": 2, "min_loss_scale": 1}, "gradient_clipping": 1.0, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "steps_per_print": 1, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 10, "hidden_size": 640, "num_attention_heads": 10, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_upper_triang_masked_softmax_fusion": true, "bias_gelu_fusion": true, "rotary_pct": 0.25, "init_method": "small_init", "output_layer_init_method": "wang_init", "gpt_j_residual": true, "output_layer_parallelism": "column", "identifier_string": "410M", "lr_decay_style": "cosine", "lr_decay_iters": 132366, "min_lr": 1.2e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.00012, "tokenizer_type": "HFTokenizer", "train_data_paths": ["data/pile/train/pile_train"], "test_data_paths": ["data/pile/test/pile_test_text_document"], "valid_data_paths": [["data/pile/val/pile_val_text_document"], ["data/slim_pajama/val/all/sp_val"]], "train_data_weights": [1.0], "valid_data_weights": [[1.0], [1.0]], "test_data_weights": [1.0], "data_impl": "mmap", "save": "checkpoints/cpt_iclr_2", "config_files": {"pythia_410m_llama_setup_resume.yml": "# Suggested data paths when using GPT-NeoX locally\n{\n  # If weight_by_num_documents is True, Builds dataset weights from a multinomial distribution over groups of data according to the number of documents in each group.\n  # WARNING: setting this to True will override any user provided weights\n  # \"weight_by_num_documents\": false,\n  # \"weighted_sampler_alpha\": 0.3,\n\n  \"tokenizer-type\": \"HFTokenizer\",\n  \"vocab-file\": \"data/20B_tokenizer.json\",\n\n  \"checkpoint_validation_with_forward_pass\": False,\n  \"use_wandb\": False,\n  # \"wandb_host\": \"https://api.wandb.ai\",\n\n  \"launcher\": \"jsrun\",\n  \"deepspeed_jsrun\": true,\n  \"num_workers\": 1,\n  \"finetune\": false,\n\n  \"save\": \"checkpoints/cpt_iclr_2\",\n  \"tensorboard-dir\": \"tensorboard/cpt_iclr_2\",\n  \"log-dir\": \"logs\",\n  \"wandb_project\": \"cpt_iclr_2\",\n}", "49M.yml": "# GPT-2 pretraining setup\n{\n  #identifier string for this config used while logging\n  \"identifier_string\": \"410M\",\n\n  # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages\n  # across the node boundaries )\n  \"pipe-parallel-size\": 1,\n  \"model-parallel-size\": 1, # one copy of the model per node\n\n  # model settings\n  \"num-layers\": 10,\n  \"hidden-size\": 640,\n  \"num-attention-heads\": 10,\n  \"seq-length\": 2048,\n  \"max-position-embeddings\": 2048,\n  \"pos-emb\": \"rotary\",\n  \"rotary-pct\": 0.25,\n  \"no-weight-tying\": true,\n  \"gpt-j-residual\": true,\n  \"output-layer-parallelism\": \"column\",\n\n  # these should provide some speedup but takes a while to build, set to true if desired\n  \"scaled-upper-triang-masked-softmax-fusion\": true,\n  \"bias-gelu-fusion\": true,\n\n  # init methods\n  \"init_method\": \"small_init\",\n  \"output_layer_init_method\": \"wang_init\",\n\n  # \"optimizer\": {\n  #   \"type\": \"Adam\",\n  #   \"params\": {\n  #     \"lr\": 3.0e-4,\n  #     \"betas\": [0.9, 0.95],\n  #     \"eps\": 1.0e-8,\n  #   }\n  # },\n  # \"min_lr\": 3.0e-5,\n\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": True,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": True,\n    \"reduce_scatter\": True,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": True,\n    \"cpu_offload\": False\n  },\n\n  # LLAMA Config\n  # batch / data settings\n  # \"train_batch_size\": 1104, #1104, #1104, #1104, #1104, #1104 # approximately 2.2M batch size across 46 nodes \n  \"train_micro_batch_size_per_gpu\": 138,\n  'gas': 4,\n  \"data-impl\": \"mmap\",\n  \"split\": \"949,50,1\",\n\n  # activation checkpointing\n  \"checkpoint-activations\": true,\n  \"checkpoint-num-layers\": 1,\n  \"partition-activations\": true,\n  \"synchronize-each-layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight-decay\": 0.1,\n  \"hidden-dropout\": 0.0,\n  \"attention-dropout\": 0.0,\n\n  # precision settings of LLaMa\n  \"fp16\": {\n    \"enabled\": true,\n    # \"type\": \"bfloat16\", # set bf16 as precision\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  # \"fp32_allreduce\": True, # without a patch to torch, bf16 models have to do the allreduce in fp32\n\n\n\n  \"distributed-backend\": \"nccl\",\n  # \"lr-decay-style\": \"cosine\",\n  # \"warmup\": 0.01,\n  \"checkpoint-factor\": 400,\n  \"eval-interval\": 100,\n  \"warup-eval-interval\": 50,\n  \"eval-iters\": 10,\n\n  # logging\n  \"log-interval\": 1,\n  \"steps_per_print\": 1,\n  \"keep-last-n-checkpoints\": 1000,\n  \"wall_clock_breakdown\": true,\n\n}\n", "pile_train.yml": "{ \n  \"train-data-paths\": [\n    \"data/pile/train/pile_train\",\n  ],\n  \"train-data-weights\": [\n    1.,\n  ],\n  \"train-dataset-name\": 'pile_train',\n  \"train-iters\": 132366,\n  \"lr-decay-iters\": 132366,\n}", "pile_slimp.yml": "{\n  \"test-data-paths\": [\"data/pile/test/pile_test_text_document\"],\n  \"test-data-weights\": [\n    1.\n  ],\n  \"valid-data-paths\": [\n    [\"data/pile/val/pile_val_text_document\"],\n    [\"data/slim_pajama/val/all/sp_val\"],\n  ],\n  \"valid-data-weights\": [\n    [1.],\n    [1.],\n  ],\n  \"val-dataset-name\": 'pile_slimp',\n}\n", "none.yml": "{\n  \"load\": \"none\",\n}", "adam_cosine_lr1-2e-4_1-2e-5_wu-001.yml": "{\n\"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 1.2e-4,\n      \"betas\": [0.9, 0.95],\n      \"eps\": 1.0e-8,\n    }\n  },\n  \"min_lr\": 1.2e-5,\n  \"lr-decay-style\": \"cosine\",\n  \"warmup\": 0.01,\n}"}, "load": "none", "checkpoint_factor": 400, "batch_size": 138, "train_iters": 132366, "eval_iters": 10, "keep_last_n_checkpoints": 1000, "eval_interval": 100, "split": "949,50,1", "vocab_file": "data/20B_tokenizer.json", "num_workers": 1, "attention_dropout": 0.0, "hidden_dropout": 0.0, "weight_decay": 0.1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "gas": 1, "clip_grad": 1.0, "dynamic_loss_scale": true, "train_dataset_name": "pile_train", "val_dataset_name": "pile_slimp", "pipe_parallel_size": 1, "world_size": 1, "is_pipe_parallel": true, "use_wandb": false, "wandb_project": "cpt_iclr_2", "log_dir": "logs", "tensorboard_dir": "tensorboard/cpt_iclr_2", "log_interval": 1, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "deepspeed_jsrun": true, "user_script": "train.py", "save_iters": [400, 800, 1200, 1600, 2000, 2400, 2800, 3200, 3600, 4000, 4400, 4800, 5200, 5600, 6000, 6400, 6800, 7200, 7600, 8000, 8400, 8800, 9200, 9600, 10000, 10400, 10800, 11200, 11600, 12000, 12400, 12800, 13200, 13600, 14000, 14400, 14800, 15200, 15600, 16000, 16400, 16800, 17200, 17600, 18000, 18400, 18800, 19200, 19600, 20000, 20400, 20800, 21200, 21600, 22000, 22400, 22800, 23200, 23600, 24000, 24400, 24800, 25200, 25600, 26000, 26400, 26800, 27200, 27600, 28000, 28400, 28800, 29200, 29600, 30000, 30400, 30800, 31200, 31600, 32000, 32400, 32800, 33200, 33600, 34000, 34400, 34800, 35200, 35600, 36000, 36400, 36800, 37200, 37600, 38000, 38400, 38800, 39200, 39600, 40000, 40400, 40800, 41200, 41600, 42000, 42400, 42800, 43200, 43600, 44000, 44400, 44800, 45200, 45600, 46000, 46400, 46800, 47200, 47600, 48000, 48400, 48800, 49200, 49600, 50000, 50400, 50800, 51200, 51600, 52000, 52400, 52800, 53200, 53600, 54000, 54400, 54800, 55200, 55600, 56000, 56400, 56800, 57200, 57600, 58000, 58400, 58800, 59200, 59600, 60000, 60400, 60800, 61200, 61600, 62000, 62400, 62800, 63200, 63600, 64000, 64400, 64800, 65200, 65600, 66000, 66400, 66800, 67200, 67600, 68000, 68400, 68800, 69200, 69600, 70000, 70400, 70800, 71200, 71600, 72000, 72400, 72800, 73200, 73600, 74000, 74400, 74800, 75200, 75600, 76000, 76400, 76800, 77200, 77600, 78000, 78400, 78800, 79200, 79600, 80000, 80400, 80800, 81200, 81600, 82000, 82400, 82800, 83200, 83600, 84000, 84400, 84800, 85200, 85600, 86000, 86400, 86800, 87200, 87600, 88000, 88400, 88800, 89200, 89600, 90000, 90400, 90800, 91200, 91600, 92000, 92400, 92800, 93200, 93600, 94000, 94400, 94800, 95200, 95600, 96000, 96400, 96800, 97200, 97600, 98000, 98400, 98800, 99200, 99600, 100000, 100400, 100800, 101200, 101600, 102000, 102400, 102800, 103200, 103600, 104000, 104400, 104800, 105200, 105600, 106000, 106400, 106800, 107200, 107600, 108000, 108400, 108800, 109200, 109600, 110000, 110400, 110800, 111200, 111600, 112000, 112400, 112800, 113200, 113600, 114000, 114400, 114800, 115200, 115600, 116000, 116400, 116800, 117200, 117600, 118000, 118400, 118800, 119200, 119600, 120000, 120400, 120800, 121200, 121600, 122000, 122400, 122800, 123200, 123600, 124000, 124400, 124800, 125200, 125600, 126000, 126400, 126800, 127200, 127600, 128000, 128400, 128800, 129200, 129600, 130000, 130400, 130800, 131200, 131600, 132000], "global_num_gpus": 2}
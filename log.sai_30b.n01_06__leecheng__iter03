+ CONFIG_FILE=./configs/sai_30B.yml
+ CONTAINER_NAME=sliuxl__stable_ckpt_v00
+ CONTAINER_NAME=yongyanr-test__leecheng-pt1_13_sai_nsys
+ DIR=/fsx/sliuxl/EleutherAI/gpt-neox
+ HOSTFILE=/fsx/sliuxl/EleutherAI/gpt-neox/hostfile
+ MPI_HOSTFILE=/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi
+ cp /fsx/sliuxl/EleutherAI/gpt-neox/hostfile /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi
+ sed -i 's/$/ slots=8/g' /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi
+ cat /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi
compute-st-worker-41 slots=8
++ cat /fsx/sliuxl/EleutherAI/gpt-neox/hostfile
++ wc -l
+ NUM_NODES=1
+ SLURM_HOSTFILE='/fsx/sliuxl/EleutherAI/gpt-neox/hostfile srun -N 1 sudo pkill -9 python'
+ JID=29740
+ sleep 3
++ cat /fsx/sliuxl/EleutherAI/gpt-neox/hostfile
++ wc -l
+ RUN_CMD='cat /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi &&         cd /fsx/sliuxl/EleutherAI/gpt-neox &&         /opt/amazon/openmpi/bin/mpirun -N 8 --hostfile /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi         --mca pml ^cm --mca plm_rsh_no_tree_spawn 1 --mca btl_tcp_if_exclude lo,docker0         --mca plm_rsh_num_concurrent 1        --allow-run-as-root --mca btl_vader_single_copy_mechanism none --oversubscribe --tag-output         -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x FI_EFA_USE_DEVICE_RDMA=1         -x NCCL_SOCKET_IFNAME=^lo,docker0 -x NCCL_ALGO=ring -x NCCL_PROTO=simple         -x LD_LIBRARY_PATH -x PATH         /opt/conda/bin/python train_mpi.py --conf ./configs/sai_30B.yml'
+ trap 'scancel -n hb_29740; scancel -n train_29740' EXIT
+ SLURM_HOSTFILE=/fsx/sliuxl/EleutherAI/gpt-neox/hostfile
+ srun --job-name=hb_29740 --quit-on-interrupt --quiet sleep infinity
+ SLURM_HOSTFILE=/fsx/sliuxl/EleutherAI/gpt-neox/hostfile
+ srun --job-name=train_29740 -N 1 docker exec yongyanr-test__leecheng-pt1_13_sai_nsys bash -c 'cat /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi &&         cd /fsx/sliuxl/EleutherAI/gpt-neox &&         /opt/amazon/openmpi/bin/mpirun -N 8 --hostfile /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi         --mca pml ^cm --mca plm_rsh_no_tree_spawn 1 --mca btl_tcp_if_exclude lo,docker0         --mca plm_rsh_num_concurrent 1        --allow-run-as-root --mca btl_vader_single_copy_mechanism none --oversubscribe --tag-output         -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x FI_EFA_USE_DEVICE_RDMA=1         -x NCCL_SOCKET_IFNAME=^lo,docker0 -x NCCL_ALGO=ring -x NCCL_PROTO=simple         -x LD_LIBRARY_PATH -x PATH         /opt/conda/bin/python train_mpi.py --conf ./configs/sai_30B.yml'
compute-st-worker-41 slots=8
[1,4]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,4]<stderr>:To add an exception for this directory, call:
[1,4]<stderr>:
[1,4]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,7]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,7]<stderr>:To add an exception for this directory, call:
[1,7]<stderr>:
[1,7]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,5]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,5]<stderr>:To add an exception for this directory, call:
[1,5]<stderr>:
[1,5]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,0]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,0]<stderr>:To add an exception for this directory, call:
[1,0]<stderr>:
[1,0]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,6]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,6]<stderr>:To add an exception for this directory, call:
[1,6]<stderr>:
[1,6]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,1]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,1]<stderr>:To add an exception for this directory, call:
[1,1]<stderr>:
[1,1]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,2]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,2]<stderr>:To add an exception for this directory, call:
[1,2]<stderr>:
[1,2]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,3]<stderr>:fatal: detected dubious ownership in repository at '/fsx/sliuxl/EleutherAI/gpt-neox'
[1,3]<stderr>:To add an exception for this directory, call:
[1,3]<stderr>:
[1,3]<stderr>:	git config --global --add safe.directory /fsx/sliuxl/EleutherAI/gpt-neox
[1,5]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,5]<stdout>:-------------------- arguments --------------------
[1,5]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,5]<stdout>:  attention_dropout ............... 0...........................updated
[1,5]<stdout>:  batch_size ...................... 16..........................updated
[1,5]<stdout>:  bias_gelu_fusion ................ True........................updated
[1,5]<stdout>:  checkpoint_activations .......... True........................updated
[1,5]<stdout>:  checkpoint_factor ............... 1000........................updated[1,5]<stdout>:
[1,5]<stdout>:  clip_grad ....................... 1.0.........................updated
[1,5]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,5]<stdout>:  data_impl ....................... mmap........................updated
[1,5]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,5]<stdout>:  deepspeed_mpi ................... True........................updated
[1,5]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,5]<stdout>:  eval_interval ................... 4000........................updated[1,5]<stdout>:
[1,5]<stdout>:  eval_iters ...................... 10..........................updated
[1,5]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated[1,5]<stdout>:
[1,5]<stdout>:  gas ............................. 1...........................updated[1,5]<stdout>:
[1,5]<stdout>:  global_num_gpus ................. 8...........................updated
[1,5]<stdout>:  gpt_j_residual .................. True........................updated
[1,5]<stdout>:  gradient_clipping ............... 1.0.........................updated
[1,5]<stdout>:  hidden_dropout .................. 0...........................updated
[1,5]<stdout>:  hidden_size ..................... 12288.......................updated
[1,5]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated
[1,5]<stdout>:  log_interval .................... 10..........................updated
[1,5]<stdout>:  lr .............................. 0.0006......................updated
[1,5]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,5]<stdout>:  lr_decay_style .................. cosine......................updated
[1,5]<stdout>:  max_position_embeddings ......... 4096........................updated
[1,5]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,5]<stdout>:  min_lr .......................... 6e-05.......................updated
[1,5]<stdout>:  model_parallel_size ............. 8...........................updated
[1,5]<stdout>:  no_weight_tying ................. True........................updated
[1,5]<stdout>:  num_attention_heads ............. 96..........................updated
[1,5]<stdout>:  num_layers ...................... 4...........................updated
[1,5]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated[1,5]<stdout>:
[1,5]<stdout>:  optimizer_type .................. Adam........................updated
[1,5]<stdout>:  output_layer_parallelism ........ column......................updated
[1,5]<stdout>:  partition_activations ........... True........................updated
[1,5]<stdout>:  pipe_parallel_size .............. 1...........................updated[1,5]<stdout>:
[1,5]<stdout>:  pos_emb ......................... rotary......................updated
[1,5]<stdout>:  precision ....................... fp16........................updated
[1,5]<stdout>:  rotary_pct ...................... 0.25........................updated[1,5]<stdout>:
[1,5]<stdout>:  save_iters ...................... []..........................updated[1,5]<stdout>:
[1,5]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,5]<stdout>:  seq_length ...................... 4096........................updated[1,5]<stdout>:
[1,5]<stdout>:  sparsity_config ................. {}..........................updated
[1,5]<stdout>:  synchronize_each_layer .......... True........................updated
[1,5]<stdout>:  text_gen_type ................... unconditional...............updated
[1,5]<stdout>:  train_batch_size ................ 16..........................updated[1,5]<stdout>:
[1,5]<stdout>:  train_iters ..................... 3...........................updated[1,5]<stdout>:
[1,5]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated
[1,5]<stdout>:  use_wandb ....................... False.......................updated
[1,5]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,5]<stdout>:  wall_clock_breakdown ............ True........................updated
[1,5]<stdout>:  weight_decay .................... 0.2.........................updated
[1,5]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,5]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,5]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
[1,5]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,5]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,5]<stdout>:  zero_stage ...................... 3...........................updated
[1,5]<stdout>:  activation ...................... gelu........................default
[1,5]<stdout>:  adlr_autoresume ................. False.......................default
[1,5]<stdout>:  adlr_autoresume_interval ........ 1000........................default
[1,5]<stdout>:  amp ............................. None........................default
[1,5]<stdout>:  apply_query_key_layer_scaling ... False.......................default
[1,5]<stdout>:  attention_softmax_in_fp32 ....... False.......................default
[1,5]<stdout>:  autotuning ...................... None........................default
[1,5]<stdout>:  autotuning_run .................. None........................default
[1,5]<stdout>:  base_shapes_file ................ None........................default
[1,5]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,5]<stdout>:  char_level_ppl .................. False.......................default
[1,5]<stdout>:  checkpoint_in_cpu ............... False.......................default[1,5]<stdout>:
[1,5]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,5]<stdout>:  checkpoint_scale ................ linear......................default
[1,5]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,5]<stdout>:  comment ......................... None........................default
[1,5]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,5]<stdout>:  coord_check ..................... False.......................default
[1,5]<stdout>:  curriculum_learning ............. None........................default
[1,5]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,5]<stdout>:  deepscale ....................... False.......................default
[1,5]<stdout>:  deepscale_config ................ None........................default
[1,5]<stdout>:  deepspeed ....................... True........................default
[1,5]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,5]<stdout>:  deepspeed_slurm ................. False.......................default
[1,5]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,5]<stdout>:  distributed_backend ............. nccl........................default
[1,5]<stdout>:  do_test ......................... None........................default
[1,5]<stdout>:  do_train ........................ None........................default
[1,5]<stdout>:  do_valid ........................ None........................default
[1,5]<stdout>:  dump_state ...................... False.......................default
[1,5]<stdout>:  eod_mask_loss ................... False.......................default
[1,5]<stdout>:  eval_results_prefix ............. ............................default
[1,5]<stdout>:  eval_tasks ...................... None........................default
[1,5]<stdout>:  exclude ......................... None........................default
[1,5]<stdout>:  exit_interval ................... None........................default
[1,5]<stdout>:  extra_save_iters ................ None........................default
[1,5]<stdout>:  finetune ........................ False.......................default
[1,5]<stdout>:  flops_profiler .................. None........................default
[1,5]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,5]<stdout>:  fp32_allreduce .................. False.......................default
[1,5]<stdout>:  git_hash ........................ None........................default
[1,5]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,5]<stdout>:  gpt_j_tied ...................... False.......................default
[1,5]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,5]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default
[1,5]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default
[1,5]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,5]<stdout>:  hysteresis ...................... 2...........................default
[1,5]<stdout>:  include ......................... None........................default
[1,5]<stdout>:  init_method ..................... normal......................default
[1,5]<stdout>:  init_method_std ................. 0.02........................default
[1,5]<stdout>:  is_pipe_parallel ................ False.......................default
[1,5]<stdout>:  iteration ....................... None........................default
[1,5]<stdout>:  keep_last_n_checkpoints ......... None........................default[1,5]<stdout>:
[1,5]<stdout>:  launcher ........................ pdsh........................default
[1,5]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,5]<stdout>:  lazy_mpu_init ................... False.......................default
[1,5]<stdout>:  load ............................ None........................default
[1,5]<stdout>:  local_rank ...................... None........................default
[1,5]<stdout>:  log_dir ......................... None........................default
[1,5]<stdout>:  log_grad_norm ................... False.......................default
[1,5]<stdout>:  log_grad_pct_zeros .............. False.......................default
[1,5]<stdout>:  log_gradient_noise_scale ........ False.......................default
[1,5]<stdout>:  log_optimizer_states ............ False.......................default
[1,5]<stdout>:  log_param_norm .................. False.......................default
[1,5]<stdout>:  loss_scale ...................... None........................default
[1,5]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,5]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,5]<stdout>:  master_addr ..................... None........................default
[1,5]<stdout>:  master_port ..................... 29500.......................default
[1,5]<stdout>:  maximum_tokens .................. 64..........................default
[1,5]<stdout>:  min_scale ....................... 1.0.........................default
[1,5]<stdout>:  mmap_warmup ..................... False.......................default
[1,5]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,5]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,5]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,5]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,5]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default
[1,5]<stdout>:  mup_width_scale ................. 2...........................default
[1,5]<stdout>:  no_load_optim ................... False.......................default
[1,5]<stdout>:  no_load_rng ..................... False.......................default[1,5]<stdout>:
[1,5]<stdout>:  no_save_optim ................... False.......................default
[1,5]<stdout>:  no_save_rng ..................... False.......................default
[1,5]<stdout>:  no_ssh_check .................... False.......................default
[1,5]<stdout>:  norm ............................ layernorm...................default
[1,5]<stdout>:  num_gpus ........................ None........................default
[1,5]<stdout>:  num_nodes ....................... -1..........................default
[1,5]<stdout>:  num_samples ..................... 1...........................default
[1,5]<stdout>:  num_unique_layers ............... None........................default
[1,5]<stdout>:  num_workers ..................... 2...........................default[1,5]<stdout>:
[1,5]<stdout>:  onnx_safe ....................... False.......................default[1,5]<stdout>:
[1,5]<stdout>:  opt_pos_emb_offset .............. 0...........................default[1,5]<stdout>:
[1,5]<stdout>:  output_layer_init_method ........ scaled_normal...............default
[1,5]<stdout>:  override_lr_scheduler ........... False.......................default
[1,5]<stdout>:  padded_vocab_size ............... None........................default
[1,5]<stdout>:  param_sharing_style ............. grouped.....................default
[1,5]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,5]<stdout>:  prescale_gradients .............. False.......................default
[1,5]<stdout>:  profile_backward ................ False.......................default[1,5]<stdout>:
[1,5]<stdout>:  prompt_end ...................... 
[1,5]<stdout>:...........................default[1,5]<stdout>:
[1,5]<stdout>:  rank ............................ None........................default
[1,5]<stdout>:  recompute ....................... False.......................default
[1,5]<stdout>:  return_logits ................... False.......................default
[1,5]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,5]<stdout>:  rotary_emb_base ................. 10000.......................default
[1,5]<stdout>:  rpe_max_distance ................ 128.........................default
[1,5]<stdout>:  rpe_num_buckets ................. 32..........................default
[1,5]<stdout>:  sample_input_file ............... None........................default[1,5]<stdout>:
[1,5]<stdout>:  sample_output_file .............. samples.txt.................default
[1,5]<stdout>:  save ............................ None........................default[1,5]<stdout>:
[1,5]<stdout>:  save_base_shapes ................ False.......................default
[1,5]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,5]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,5]<stdout>:  scheduler ....................... None........................default
[1,5]<stdout>:  seed ............................ 1234........................default[1,5]<stdout>:
[1,5]<stdout>:  short_seq_prob .................. 0.1.........................default[1,5]<stdout>:
[1,5]<stdout>:  soft_prompt_tuning .............. None........................default
[1,5]<stdout>:  sparse_gradients ................ False.......................default
[1,5]<stdout>:  split ........................... 969, 30, 1..................default
[1,5]<stdout>:  steps_per_print ................. 10..........................default[1,5]<stdout>:
[1,5]<stdout>:  temperature ..................... 0.0.........................default[1,5]<stdout>:
[1,5]<stdout>:  tensorboard_dir ................. None........................default
[1,5]<stdout>:  test_data_paths ................. None........................default
[1,5]<stdout>:  test_data_weights ............... None........................default
[1,5]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default[1,5]<stdout>:
[1,5]<stdout>:  top_k ........................... 0...........................default[1,5]<stdout>:
[1,5]<stdout>:  top_p ........................... 0.0.........................default[1,5]<stdout>:
[1,5]<stdout>:  train_data_paths ................ None........................default
[1,5]<stdout>:  train_data_weights .............. None........................default
[1,5]<stdout>:  use_bnb_optimizer ............... False.......................default[1,5]<stdout>:
[1,5]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default[1,5]<stdout>:
[1,5]<stdout>:  use_cpu_initialization .......... False.......................default[1,5]<stdout>:
[1,5]<stdout>:  use_mup ......................... False.......................default
[1,5]<stdout>:  use_shared_fs ................... True........................default
[1,5]<stdout>:  user_script ..................... None........................default
[1,5]<stdout>:  valid_data_paths ................ None........................default[1,5]<stdout>:
[1,5]<stdout>:  valid_data_weights .............. None........................default
[1,5]<stdout>:  wandb_group ..................... None........................default
[1,5]<stdout>:  wandb_host ...................... https://api.wandb.ai........default[1,5]<stdout>:
[1,5]<stdout>:  wandb_init_all_ranks ............ False.......................default[1,5]<stdout>:
[1,5]<stdout>:  wandb_project ................... neox........................default[1,5]<stdout>:
[1,5]<stdout>:  wandb_team ...................... None........................default[1,5]<stdout>:
[1,5]<stdout>:  warmup .......................... 0.01........................default
[1,5]<stdout>:  weight_by_num_documents ......... False.......................default
[1,5]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,5]<stdout>:  world_size ...................... None........................default
[1,5]<stdout>:  zero_allow_untested_optimizer ... False.......................default[1,5]<stdout>:
[1,5]<stdout>:---------------- end of arguments ----------------
[1,4]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,7]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,4]<stdout>:-------------------- arguments --------------------
[1,4]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,4]<stdout>:  attention_dropout ............... 0...........................updated
[1,4]<stdout>:  batch_size ...................... 16..........................updated
[1,4]<stdout>:  bias_gelu_fusion ................ True........................updated[1,4]<stdout>:
[1,4]<stdout>:  checkpoint_activations .......... True........................updated
[1,4]<stdout>:  checkpoint_factor ............... 1000........................updated
[1,4]<stdout>:  clip_grad ....................... 1.0.........................updated
[1,4]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,4]<stdout>:  data_impl ....................... mmap........................updated
[1,4]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,4]<stdout>:  deepspeed_mpi ................... True........................updated
[1,4]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,4]<stdout>:  eval_interval ................... 4000........................updated
[1,4]<stdout>:  eval_iters ...................... 10..........................updated
[1,4]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
[1,4]<stdout>:  gas ............................. 1...........................updated
[1,4]<stdout>:  global_num_gpus ................. 8...........................updated
[1,4]<stdout>:  gpt_j_residual .................. True........................updated
[1,4]<stdout>:  gradient_clipping ............... 1.0.........................updated
[1,4]<stdout>:  hidden_dropout .................. 0...........................updated
[1,4]<stdout>:  hidden_size ..................... 12288.......................updated
[1,4]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated
[1,4]<stdout>:  log_interval .................... 10..........................updated
[1,4]<stdout>:  lr .............................. 0.0006......................updated
[1,4]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,4]<stdout>:  lr_decay_style .................. cosine......................updated
[1,4]<stdout>:  max_position_embeddings ......... 4096........................updated
[1,4]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,4]<stdout>:  min_lr .......................... 6e-05.......................updated
[1,4]<stdout>:  model_parallel_size ............. 8...........................updated
[1,4]<stdout>:  no_weight_tying ................. True........................updated
[1,4]<stdout>:  num_attention_heads ............. 96..........................updated
[1,4]<stdout>:  num_layers ...................... 4...........................updated
[1,4]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated
[1,4]<stdout>:  optimizer_type .................. Adam........................updated
[1,4]<stdout>:  output_layer_parallelism ........ column......................updated
[1,4]<stdout>:  partition_activations ........... True........................updated
[1,4]<stdout>:  pipe_parallel_size .............. 1...........................updated
[1,4]<stdout>:  pos_emb ......................... rotary......................updated
[1,4]<stdout>:  precision ....................... fp16........................updated
[1,4]<stdout>:  rotary_pct ...................... 0.25........................updated
[1,4]<stdout>:  save_iters ...................... []..........................updated
[1,4]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,4]<stdout>:  seq_length ...................... 4096........................updated
[1,4]<stdout>:  sparsity_config ................. {}..........................updated
[1,4]<stdout>:  synchronize_each_layer .......... True........................updated
[1,4]<stdout>:  text_gen_type ................... unconditional...............updated
[1,4]<stdout>:  train_batch_size ................ 16..........................updated
[1,4]<stdout>:  train_iters ..................... 3...........................updated
[1,4]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated
[1,4]<stdout>:  use_wandb ....................... False.......................updated
[1,4]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,4]<stdout>:  wall_clock_breakdown ............ True........................updated
[1,4]<stdout>:  weight_decay .................... 0.2.........................updated
[1,4]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,4]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,4]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
[1,4]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,4]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,4]<stdout>:  zero_stage ...................... 3...........................updated
[1,4]<stdout>:  activation ...................... gelu........................default
[1,4]<stdout>:  adlr_autoresume ................. False.......................default
[1,4]<stdout>:  adlr_autoresume_interval ........ 1000........................default
[1,4]<stdout>:  amp ............................. None........................default
[1,4]<stdout>:  apply_query_key_layer_scaling ... False.......................default
[1,4]<stdout>:  attention_softmax_in_fp32 ....... False.......................default
[1,4]<stdout>:  autotuning ...................... None........................default
[1,4]<stdout>:  autotuning_run .................. None........................default
[1,4]<stdout>:  base_shapes_file ................ None........................default
[1,4]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,4]<stdout>:  char_level_ppl .................. False.......................default
[1,4]<stdout>:  checkpoint_in_cpu ............... False.......................default
[1,4]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,4]<stdout>:  checkpoint_scale ................ linear......................default
[1,4]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,4]<stdout>:  comment ......................... None........................default
[1,4]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,4]<stdout>:  coord_check ..................... False.......................default
[1,4]<stdout>:  curriculum_learning ............. None........................default
[1,4]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,4]<stdout>:  deepscale ....................... False.......................default
[1,4]<stdout>:  deepscale_config ................ None........................default
[1,4]<stdout>:  deepspeed ....................... True........................default
[1,4]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,4]<stdout>:  deepspeed_slurm ................. False.......................default
[1,4]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,4]<stdout>:  distributed_backend ............. nccl........................default
[1,4]<stdout>:  do_test ......................... None........................default
[1,4]<stdout>:  do_train ........................ None........................default
[1,4]<stdout>:  do_valid ........................ None........................default
[1,4]<stdout>:  dump_state ...................... False.......................default
[1,4]<stdout>:  eod_mask_loss ................... False.......................default
[1,4]<stdout>:  eval_results_prefix ............. ............................default
[1,4]<stdout>:  eval_tasks ...................... None........................default
[1,4]<stdout>:  exclude ......................... None........................default
[1,4]<stdout>:  exit_interval ................... None........................default
[1,4]<stdout>:  extra_save_iters ................ None........................default
[1,4]<stdout>:  finetune ........................ False.......................default
[1,4]<stdout>:  flops_profiler .................. None........................default
[1,4]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,4]<stdout>:  fp32_allreduce .................. False.......................default
[1,4]<stdout>:  git_hash ........................ None........................default
[1,4]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,4]<stdout>:  gpt_j_tied ...................... False.......................default
[1,4]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,4]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default[1,4]<stdout>:
[1,4]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default
[1,4]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,4]<stdout>:  hysteresis ...................... 2...........................default
[1,4]<stdout>:  include ......................... None........................default
[1,4]<stdout>:  init_method ..................... normal......................default
[1,4]<stdout>:  init_method_std ................. 0.02........................default
[1,4]<stdout>:  is_pipe_parallel ................ False.......................default
[1,4]<stdout>:  iteration ....................... None........................default
[1,4]<stdout>:  keep_last_n_checkpoints ......... None........................default
[1,4]<stdout>:  launcher ........................ pdsh........................default
[1,4]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,4]<stdout>:  lazy_mpu_init ................... False.......................default
[1,4]<stdout>:  load ............................ None........................default
[1,4]<stdout>:  local_rank ...................... None........................default
[1,4]<stdout>:  log_dir ......................... None........................default
[1,4]<stdout>:  log_grad_norm ................... False.......................default
[1,4]<stdout>:  log_grad_pct_zeros .............. False.......................default
[1,4]<stdout>:  log_gradient_noise_scale ........ False.......................default
[1,4]<stdout>:  log_optimizer_states ............ False.......................default
[1,4]<stdout>:  log_param_norm .................. False.......................default
[1,4]<stdout>:  loss_scale ...................... None........................default
[1,4]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,4]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,4]<stdout>:  master_addr ..................... None........................default
[1,4]<stdout>:  master_port ..................... 29500.......................default
[1,4]<stdout>:  maximum_tokens .................. 64..........................default
[1,4]<stdout>:  min_scale ....................... 1.0.........................default
[1,4]<stdout>:  mmap_warmup ..................... False.......................default
[1,4]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,4]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,4]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,4]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,4]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default
[1,4]<stdout>:  mup_width_scale ................. 2...........................default
[1,4]<stdout>:  no_load_optim ................... False.......................default
[1,4]<stdout>:  no_load_rng ..................... False.......................default
[1,4]<stdout>:  no_save_optim ................... False.......................default
[1,4]<stdout>:  no_save_rng ..................... False.......................default
[1,4]<stdout>:  no_ssh_check .................... False.......................default
[1,4]<stdout>:  norm ............................ layernorm...................default
[1,4]<stdout>:  num_gpus ........................ None........................default
[1,4]<stdout>:  num_nodes ....................... -1..........................default
[1,4]<stdout>:  num_samples ..................... 1...........................default
[1,4]<stdout>:  num_unique_layers ............... None........................default
[1,4]<stdout>:  num_workers ..................... 2...........................default
[1,4]<stdout>:  onnx_safe ....................... False.......................default
[1,4]<stdout>:  opt_pos_emb_offset .............. 0...........................default
[1,4]<stdout>:  output_layer_init_method ........ scaled_normal...............default
[1,4]<stdout>:  override_lr_scheduler ........... False.......................default
[1,4]<stdout>:  padded_vocab_size ............... None........................default
[1,4]<stdout>:  param_sharing_style ............. grouped.....................default
[1,4]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,4]<stdout>:  prescale_gradients .............. False.......................default
[1,4]<stdout>:  profile_backward ................ False.......................default
[1,4]<stdout>:  prompt_end ...................... 
[1,4]<stdout>:...........................default
[1,4]<stdout>:  rank ............................ None........................default
[1,4]<stdout>:  recompute ....................... False.......................default
[1,4]<stdout>:  return_logits ................... False.......................default
[1,4]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,4]<stdout>:  rotary_emb_base ................. 10000.......................default
[1,4]<stdout>:  rpe_max_distance ................ 128.........................default
[1,4]<stdout>:  rpe_num_buckets ................. 32..........................default
[1,4]<stdout>:  sample_input_file ............... None........................default
[1,4]<stdout>:  sample_output_file .............. samples.txt.................default
[1,4]<stdout>:  save ............................ None........................default
[1,4]<stdout>:  save_base_shapes ................ False.......................default
[1,4]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,4]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,4]<stdout>:  scheduler ....................... None........................default
[1,4]<stdout>:  seed ............................ 1234........................default
[1,4]<stdout>:  short_seq_prob .................. 0.1.........................default
[1,4]<stdout>:  soft_prompt_tuning .............. None........................default
[1,4]<stdout>:  sparse_gradients ................ False.......................default
[1,4]<stdout>:  split ........................... 969, 30, 1..................default
[1,4]<stdout>:  steps_per_print ................. 10..........................default
[1,4]<stdout>:  temperature ..................... 0.0.........................default
[1,4]<stdout>:  tensorboard_dir ................. None........................default
[1,4]<stdout>:  test_data_paths ................. None........................default
[1,4]<stdout>:  test_data_weights ............... None........................default
[1,4]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default
[1,4]<stdout>:  top_k ........................... 0...........................default
[1,4]<stdout>:  top_p ........................... 0.0.........................default
[1,4]<stdout>:  train_data_paths ................ None........................default
[1,4]<stdout>:  train_data_weights .............. None........................default
[1,4]<stdout>:  use_bnb_optimizer ............... False.......................default
[1,4]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default
[1,4]<stdout>:  use_cpu_initialization .......... False.......................default
[1,4]<stdout>:  use_mup ......................... False.......................default
[1,4]<stdout>:  use_shared_fs ................... True........................default
[1,4]<stdout>:  user_script ..................... None........................default
[1,4]<stdout>:  valid_data_paths ................ None........................default
[1,4]<stdout>:  valid_data_weights .............. None........................default
[1,4]<stdout>:  wandb_group ..................... None........................default
[1,4]<stdout>:  wandb_host ...................... https://api.wandb.ai........default
[1,4]<stdout>:  wandb_init_all_ranks ............ False.......................default
[1,4]<stdout>:  wandb_project ................... neox........................default
[1,4]<stdout>:  wandb_team ...................... None........................default
[1,4]<stdout>:  warmup .......................... 0.01........................default
[1,4]<stdout>:  weight_by_num_documents ......... False.......................default
[1,4]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,4]<stdout>:  world_size ...................... None........................default
[1,4]<stdout>:  zero_allow_untested_optimizer ... False.......................default
[1,4]<stdout>:---------------- end of arguments ----------------
[1,7]<stdout>:-------------------- arguments --------------------
[1,7]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,7]<stdout>:  attention_dropout ............... 0...........................updated
[1,7]<stdout>:  batch_size ...................... 16..........................updated
[1,7]<stdout>:  bias_gelu_fusion ................ True........................updated
[1,7]<stdout>:  checkpoint_activations .......... True........................updated
[1,7]<stdout>:  checkpoint_factor ............... 1000........................updated[1,7]<stdout>:
[1,7]<stdout>:  clip_grad ....................... 1.0.........................updated
[1,7]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,7]<stdout>:  data_impl ....................... mmap........................updated
[1,7]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,7]<stdout>:  deepspeed_mpi ................... True........................updated
[1,7]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,7]<stdout>:  eval_interval ................... 4000........................updated
[1,7]<stdout>:  eval_iters ...................... 10..........................updated
[1,7]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
[1,7]<stdout>:  gas ............................. 1...........................updated
[1,7]<stdout>:  global_num_gpus ................. 8...........................updated[1,7]<stdout>:
[1,7]<stdout>:  gpt_j_residual .................. True........................updated
[1,7]<stdout>:  gradient_clipping ............... 1.0.........................updated[1,7]<stdout>:
[1,7]<stdout>:  hidden_dropout .................. 0...........................updated
[1,7]<stdout>:  hidden_size ..................... 12288.......................updated
[1,7]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated
[1,7]<stdout>:  log_interval .................... 10..........................updated
[1,7]<stdout>:  lr .............................. 0.0006......................updated
[1,7]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,7]<stdout>:  lr_decay_style .................. cosine......................updated
[1,7]<stdout>:  max_position_embeddings ......... 4096........................updated
[1,7]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,7]<stdout>:  min_lr .......................... 6e-05.......................updated
[1,7]<stdout>:  model_parallel_size ............. 8...........................updated
[1,7]<stdout>:  no_weight_tying ................. True........................updated
[1,7]<stdout>:  num_attention_heads ............. 96..........................updated
[1,7]<stdout>:  num_layers ...................... 4...........................updated
[1,7]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated
[1,7]<stdout>:  optimizer_type .................. Adam........................updated
[1,7]<stdout>:  output_layer_parallelism ........ column......................updated
[1,7]<stdout>:  partition_activations ........... True........................updated
[1,7]<stdout>:  pipe_parallel_size .............. 1...........................updated
[1,7]<stdout>:  pos_emb ......................... rotary......................updated
[1,7]<stdout>:  precision ....................... fp16........................updated
[1,7]<stdout>:  rotary_pct ...................... 0.25........................updated
[1,7]<stdout>:  save_iters ...................... []..........................updated
[1,7]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,7]<stdout>:  seq_length ...................... 4096........................updated
[1,7]<stdout>:  sparsity_config ................. {}..........................updated
[1,7]<stdout>:  synchronize_each_layer .......... True........................updated
[1,7]<stdout>:  text_gen_type ................... unconditional...............updated
[1,7]<stdout>:  train_batch_size ................ 16..........................updated
[1,7]<stdout>:  train_iters ..................... 3...........................updated
[1,7]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated
[1,7]<stdout>:  use_wandb ....................... False.......................updated
[1,7]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,7]<stdout>:  wall_clock_breakdown ............ True........................updated
[1,7]<stdout>:  weight_decay .................... 0.2.........................updated
[1,7]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,7]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,7]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
[1,7]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,7]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,7]<stdout>:  zero_stage ...................... 3...........................updated
[1,7]<stdout>:  activation ...................... gelu........................default
[1,7]<stdout>:  adlr_autoresume ................. False.......................default
[1,7]<stdout>:  adlr_autoresume_interval ........ 1000........................default
[1,7]<stdout>:  amp ............................. None........................default
[1,7]<stdout>:  apply_query_key_layer_scaling ... False.......................default
[1,7]<stdout>:  attention_softmax_in_fp32 ....... False.......................default
[1,7]<stdout>:  autotuning ...................... None........................default
[1,7]<stdout>:  autotuning_run .................. None........................default
[1,7]<stdout>:  base_shapes_file ................ None........................default
[1,7]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,7]<stdout>:  char_level_ppl .................. False.......................default
[1,7]<stdout>:  checkpoint_in_cpu ............... False.......................default
[1,7]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,7]<stdout>:  checkpoint_scale ................ linear......................default
[1,7]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,7]<stdout>:  comment ......................... None........................default
[1,7]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,7]<stdout>:  coord_check ..................... False.......................default
[1,7]<stdout>:  curriculum_learning ............. None........................default
[1,7]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,7]<stdout>:  deepscale ....................... False.......................default
[1,7]<stdout>:  deepscale_config ................ None........................default
[1,7]<stdout>:  deepspeed ....................... True........................default
[1,7]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,7]<stdout>:  deepspeed_slurm ................. False.......................default
[1,7]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,7]<stdout>:  distributed_backend ............. nccl........................default
[1,7]<stdout>:  do_test ......................... None........................default
[1,7]<stdout>:  do_train ........................ None........................default
[1,7]<stdout>:  do_valid ........................ None........................default
[1,7]<stdout>:  dump_state ...................... False.......................default
[1,7]<stdout>:  eod_mask_loss ................... False.......................default
[1,7]<stdout>:  eval_results_prefix ............. ............................default
[1,7]<stdout>:  eval_tasks ...................... None........................default
[1,7]<stdout>:  exclude ......................... None........................default
[1,7]<stdout>:  exit_interval ................... None........................default
[1,7]<stdout>:  extra_save_iters ................ None........................default
[1,7]<stdout>:  finetune ........................ False.......................default
[1,7]<stdout>:  flops_profiler .................. None........................default
[1,7]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,7]<stdout>:  fp32_allreduce .................. False.......................default
[1,7]<stdout>:  git_hash ........................ None........................default
[1,7]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,7]<stdout>:  gpt_j_tied ...................... False.......................default
[1,7]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,7]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default
[1,7]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default
[1,7]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,7]<stdout>:  hysteresis ...................... 2...........................default
[1,7]<stdout>:  include ......................... None........................default
[1,7]<stdout>:  init_method ..................... normal......................default[1,7]<stdout>:
[1,7]<stdout>:  init_method_std ................. 0.02........................default
[1,7]<stdout>:  is_pipe_parallel ................ False.......................default
[1,7]<stdout>:  iteration ....................... None........................default
[1,7]<stdout>:  keep_last_n_checkpoints ......... None........................default
[1,7]<stdout>:  launcher ........................ pdsh........................default
[1,7]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,7]<stdout>:  lazy_mpu_init ................... False.......................default
[1,7]<stdout>:  load ............................ None........................default
[1,7]<stdout>:  local_rank ...................... None........................default
[1,7]<stdout>:  log_dir ......................... None........................default
[1,7]<stdout>:  log_grad_norm ................... False.......................default
[1,7]<stdout>:  log_grad_pct_zeros .............. False.......................default
[1,7]<stdout>:  log_gradient_noise_scale ........ False.......................default
[1,7]<stdout>:  log_optimizer_states ............ False.......................default
[1,7]<stdout>:  log_param_norm .................. False.......................default
[1,7]<stdout>:  loss_scale ...................... None........................default
[1,7]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,7]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,7]<stdout>:  master_addr ..................... None........................default
[1,7]<stdout>:  master_port ..................... 29500.......................default
[1,7]<stdout>:  maximum_tokens .................. 64..........................default
[1,7]<stdout>:  min_scale ....................... 1.0.........................default
[1,7]<stdout>:  mmap_warmup ..................... False.......................default
[1,7]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,7]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,7]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,7]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,7]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default
[1,7]<stdout>:  mup_width_scale ................. 2...........................default
[1,7]<stdout>:  no_load_optim ................... False.......................default
[1,7]<stdout>:  no_load_rng ..................... False.......................default
[1,7]<stdout>:  no_save_optim ................... False.......................default
[1,7]<stdout>:  no_save_rng ..................... False.......................default
[1,7]<stdout>:  no_ssh_check .................... False.......................default
[1,7]<stdout>:  norm ............................ layernorm...................default
[1,7]<stdout>:  num_gpus ........................ None........................default
[1,7]<stdout>:  num_nodes ....................... -1..........................default
[1,7]<stdout>:  num_samples ..................... 1...........................default
[1,7]<stdout>:  num_unique_layers ............... None........................default
[1,7]<stdout>:  num_workers ..................... 2...........................default
[1,7]<stdout>:  onnx_safe ....................... False.......................default
[1,7]<stdout>:  opt_pos_emb_offset .............. 0...........................default
[1,7]<stdout>:  output_layer_init_method ........ scaled_normal...............default
[1,7]<stdout>:  override_lr_scheduler ........... False.......................default
[1,7]<stdout>:  padded_vocab_size ............... None........................default
[1,7]<stdout>:  param_sharing_style ............. grouped.....................default
[1,7]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,7]<stdout>:  prescale_gradients .............. False.......................default
[1,7]<stdout>:  profile_backward ................ False.......................default
[1,7]<stdout>:  prompt_end ...................... 
[1,7]<stdout>:...........................default
[1,7]<stdout>:  rank ............................ None........................default
[1,7]<stdout>:  recompute ....................... False.......................default
[1,7]<stdout>:  return_logits ................... False.......................default
[1,7]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,7]<stdout>:  rotary_emb_base ................. 10000.......................default
[1,7]<stdout>:  rpe_max_distance ................ 128.........................default
[1,7]<stdout>:  rpe_num_buckets ................. 32..........................default[1,7]<stdout>:
[1,7]<stdout>:  sample_input_file ............... None........................default
[1,7]<stdout>:  sample_output_file .............. samples.txt.................default
[1,7]<stdout>:  save ............................ None........................default
[1,7]<stdout>:  save_base_shapes ................ False.......................default
[1,7]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,7]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,7]<stdout>:  scheduler ....................... None........................default
[1,7]<stdout>:  seed ............................ 1234........................default
[1,7]<stdout>:  short_seq_prob .................. 0.1.........................default
[1,7]<stdout>:  soft_prompt_tuning .............. None........................default
[1,7]<stdout>:  sparse_gradients ................ False.......................default
[1,7]<stdout>:  split ........................... 969, 30, 1..................default
[1,7]<stdout>:  steps_per_print ................. 10..........................default
[1,7]<stdout>:  temperature ..................... 0.0.........................default
[1,7]<stdout>:  tensorboard_dir ................. None........................default
[1,7]<stdout>:  test_data_paths ................. None........................default
[1,7]<stdout>:  test_data_weights ............... None........................default
[1,7]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default
[1,7]<stdout>:  top_k ........................... 0...........................default
[1,7]<stdout>:  top_p ........................... 0.0.........................default
[1,7]<stdout>:  train_data_paths ................ None........................default
[1,7]<stdout>:  train_data_weights .............. None........................default
[1,7]<stdout>:  use_bnb_optimizer ............... False.......................default
[1,7]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default
[1,7]<stdout>:  use_cpu_initialization .......... False.......................default
[1,7]<stdout>:  use_mup ......................... False.......................default
[1,7]<stdout>:  use_shared_fs ................... True........................default
[1,7]<stdout>:  user_script ..................... None........................default
[1,7]<stdout>:  valid_data_paths ................ None........................default
[1,7]<stdout>:  valid_data_weights .............. None........................default[1,7]<stdout>:
[1,7]<stdout>:  wandb_group ..................... None........................default
[1,7]<stdout>:  wandb_host ...................... https://api.wandb.ai........default
[1,7]<stdout>:  wandb_init_all_ranks ............ False.......................default
[1,7]<stdout>:  wandb_project ................... neox........................default
[1,7]<stdout>:  wandb_team ...................... None........................default
[1,7]<stdout>:  warmup .......................... 0.01........................default
[1,7]<stdout>:  weight_by_num_documents ......... False.......................default
[1,7]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,7]<stdout>:  world_size ...................... None........................default
[1,7]<stdout>:  zero_allow_untested_optimizer ... False.......................default
[1,7]<stdout>:---------------- end of arguments ----------------
[1,1]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,0]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,6]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,3]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,2]<stdout>:NeoXArgs.from_ymls() ['./configs/sai_30B.yml']
[1,1]<stdout>:-------------------- arguments --------------------
[1,1]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,1]<stdout>:  attention_dropout ............... 0...........................updated
[1,1]<stdout>:  batch_size ...................... 16..........................updated
[1,1]<stdout>:  bias_gelu_fusion ................ True........................updated
[1,1]<stdout>:  checkpoint_activations .......... True........................updated
[1,1]<stdout>:  checkpoint_factor ............... 1000........................updated
[1,1]<stdout>:  clip_grad ....................... 1.0.........................updated
[1,1]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,1]<stdout>:  data_impl ....................... mmap........................updated
[1,1]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,1]<stdout>:  deepspeed_mpi ................... True........................updated
[1,1]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,1]<stdout>:  eval_interval ................... 4000........................updated
[1,1]<stdout>:  eval_iters ...................... 10..........................updated
[1,1]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
[1,1]<stdout>:  gas ............................. 1...........................updated
[1,1]<stdout>:  global_num_gpus ................. 8...........................updated
[1,1]<stdout>:  gpt_j_residual .................. True........................updated
[1,1]<stdout>:  gradient_clipping ............... 1.0.........................updated
[1,1]<stdout>:  hidden_dropout .................. 0...........................updated
[1,1]<stdout>:  hidden_size ..................... 12288.......................updated
[1,1]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated[1,1]<stdout>:
[1,1]<stdout>:  log_interval .................... 10..........................updated
[1,1]<stdout>:  lr .............................. 0.0006......................updated[1,1]<stdout>:
[1,1]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,1]<stdout>:  lr_decay_style .................. cosine......................updated
[1,1]<stdout>:  max_position_embeddings ......... 4096........................updated
[1,1]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,1]<stdout>:  min_lr .......................... 6e-05.......................updated
[1,1]<stdout>:  model_parallel_size ............. 8...........................updated
[1,1]<stdout>:  no_weight_tying ................. True........................updated
[1,1]<stdout>:  num_attention_heads ............. 96..........................updated
[1,1]<stdout>:  num_layers ...................... 4...........................updated
[1,1]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated
[1,1]<stdout>:  optimizer_type .................. Adam........................updated
[1,1]<stdout>:  output_layer_parallelism ........ column......................updated
[1,1]<stdout>:  partition_activations ........... True........................updated
[1,1]<stdout>:  pipe_parallel_size .............. 1...........................updated
[1,1]<stdout>:  pos_emb ......................... rotary......................updated
[1,1]<stdout>:  precision ....................... fp16........................updated
[1,1]<stdout>:  rotary_pct ...................... 0.25........................updated
[1,1]<stdout>:  save_iters ...................... []..........................updated
[1,1]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,1]<stdout>:  seq_length ...................... 4096........................updated
[1,1]<stdout>:  sparsity_config ................. {}..........................updated
[1,1]<stdout>:  synchronize_each_layer .......... True........................updated
[1,0]<stdout>:-------------------- arguments --------------------
[1,1]<stdout>:  text_gen_type ................... unconditional...............updated
[1,1]<stdout>:  train_batch_size ................ 16..........................updated
[1,1]<stdout>:  train_iters ..................... 3...........................updated
[1,1]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated
[1,1]<stdout>:  use_wandb ....................... False.......................updated
[1,1]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,1]<stdout>:  wall_clock_breakdown ............ True........................updated[1,1]<stdout>:
[1,1]<stdout>:  weight_decay .................... 0.2.........................updated
[1,1]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,1]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,1]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
[1,1]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,1]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,1]<stdout>:  zero_stage ...................... 3...........................updated
[1,1]<stdout>:  activation ...................... gelu........................default
[1,1]<stdout>:  adlr_autoresume ................. False.......................default
[1,1]<stdout>:  adlr_autoresume_interval ........ 1000........................default[1,1]<stdout>:
[1,1]<stdout>:  amp ............................. None........................default
[1,1]<stdout>:  apply_query_key_layer_scaling ... False.......................default
[1,1]<stdout>:  attention_softmax_in_fp32 ....... False.......................default
[1,1]<stdout>:  autotuning ...................... None........................default
[1,1]<stdout>:  autotuning_run .................. None........................default[1,1]<stdout>:
[1,1]<stdout>:  base_shapes_file ................ None........................default
[1,1]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,1]<stdout>:  char_level_ppl .................. False.......................default
[1,1]<stdout>:  checkpoint_in_cpu ............... False.......................default
[1,1]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,1]<stdout>:  checkpoint_scale ................ linear......................default
[1,1]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,1]<stdout>:  comment ......................... None........................default
[1,1]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,1]<stdout>:  coord_check ..................... False.......................default
[1,6]<stdout>:-------------------- arguments --------------------
[1,1]<stdout>:  curriculum_learning ............. None........................default
[1,1]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,1]<stdout>:  deepscale ....................... False.......................default
[1,1]<stdout>:  deepscale_config ................ None........................default
[1,1]<stdout>:  deepspeed ....................... True........................default
[1,1]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,1]<stdout>:  deepspeed_slurm ................. False.......................default
[1,1]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,1]<stdout>:  distributed_backend ............. nccl........................default
[1,1]<stdout>:  do_test ......................... None........................default
[1,1]<stdout>:  do_train ........................ None........................default
[1,1]<stdout>:  do_valid ........................ None........................default
[1,1]<stdout>:  dump_state ...................... False.......................default
[1,1]<stdout>:  eod_mask_loss ................... False.......................default
[1,1]<stdout>:  eval_results_prefix ............. ............................default
[1,1]<stdout>:  eval_tasks ...................... None........................default
[1,1]<stdout>:  exclude ......................... None........................default
[1,1]<stdout>:  exit_interval ................... None........................default
[1,1]<stdout>:  extra_save_iters ................ None........................default
[1,1]<stdout>:  finetune ........................ False.......................default
[1,1]<stdout>:  flops_profiler .................. None........................default
[1,1]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,1]<stdout>:  fp32_allreduce .................. False.......................default[1,1]<stdout>:
[1,1]<stdout>:  git_hash ........................ None........................default
[1,1]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,1]<stdout>:  gpt_j_tied ...................... False.......................default
[1,1]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,1]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default
[1,1]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default
[1,1]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,1]<stdout>:  hysteresis ...................... 2...........................default
[1,1]<stdout>:  include ......................... None........................default
[1,1]<stdout>:  init_method ..................... normal......................default
[1,1]<stdout>:  init_method_std ................. 0.02........................default
[1,1]<stdout>:  is_pipe_parallel ................ False.......................default
[1,1]<stdout>:  iteration ....................... None........................default
[1,1]<stdout>:  keep_last_n_checkpoints ......... None........................default
[1,1]<stdout>:  launcher ........................ pdsh........................default
[1,0]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,0]<stdout>:  attention_dropout ............... 0...........................updated
[1,0]<stdout>:  batch_size ...................... 16..........................updated
[1,0]<stdout>:  bias_gelu_fusion ................ True........................updated[1,1]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,1]<stdout>:  lazy_mpu_init ................... False.......................default
[1,1]<stdout>:  load ............................ None........................default
[1,1]<stdout>:  local_rank ...................... None........................default
[1,0]<stdout>:
[1,0]<stdout>:  checkpoint_activations .......... True........................updated
[1,0]<stdout>:  checkpoint_factor ............... 1000........................updated
[1,0]<stdout>:  clip_grad ....................... 1.0.........................updated[1,1]<stdout>:  log_dir ......................... None........................default
[1,1]<stdout>:  log_grad_norm ................... False.......................default
[1,1]<stdout>:  log_grad_pct_zeros .............. False.......................default
[1,1]<stdout>:  log_gradient_noise_scale ........ False.......................default[1,0]<stdout>:
[1,0]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,0]<stdout>:  data_impl ....................... mmap........................updated[1,1]<stdout>:
[1,1]<stdout>:  log_optimizer_states ............ False.......................default
[1,1]<stdout>:  log_param_norm .................. False.......................default
[1,1]<stdout>:  loss_scale ...................... None........................default
[1,1]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,0]<stdout>:
[1,0]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,0]<stdout>:  deepspeed_mpi ................... True........................updated
[1,0]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,0]<stdout>:  eval_interval ................... 4000........................updated
[1,0]<stdout>:  eval_iters ...................... 10..........................updated
[1,0]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
[1,0]<stdout>:  gas ............................. 1...........................updated[1,1]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,1]<stdout>:  master_addr ..................... None........................default
[1,1]<stdout>:  master_port ..................... 29500.......................default
[1,1]<stdout>:  maximum_tokens .................. 64..........................default
[1,1]<stdout>:  min_scale ....................... 1.0.........................default
[1,1]<stdout>:  mmap_warmup ..................... False.......................default
[1,1]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,0]<stdout>:
[1,0]<stdout>:  global_num_gpus ................. 8...........................updated
[1,0]<stdout>:  gpt_j_residual .................. True........................updated
[1,0]<stdout>:  gradient_clipping ............... 1.0.........................updated
[1,0]<stdout>:  hidden_dropout .................. 0...........................updated
[1,1]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,1]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,1]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,0]<stdout>:  hidden_size ..................... 12288.......................updated
[1,0]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated
[1,0]<stdout>:  log_interval .................... 10..........................updated
[1,1]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default
[1,1]<stdout>:  mup_width_scale ................. 2...........................default
[1,1]<stdout>:  no_load_optim ................... False.......................default
[1,1]<stdout>:  no_load_rng ..................... False.......................default
[1,0]<stdout>:  lr .............................. 0.0006......................updated
[1,0]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,0]<stdout>:  lr_decay_style .................. cosine......................updated
[1,0]<stdout>:  max_position_embeddings ......... 4096........................updated
[1,0]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,0]<stdout>:  min_lr .......................... 6e-05.......................updated[1,1]<stdout>:  no_save_optim ................... False.......................default
[1,1]<stdout>:  no_save_rng ..................... False.......................default
[1,1]<stdout>:  no_ssh_check .................... False.......................default
[1,1]<stdout>:  norm ............................ layernorm...................default
[1,1]<stdout>:  num_gpus ........................ None........................default
[1,6]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,6]<stdout>:  attention_dropout ............... 0...........................updated
[1,6]<stdout>:  batch_size ...................... 16..........................updated
[1,6]<stdout>:  bias_gelu_fusion ................ True........................updated[1,0]<stdout>:
[1,0]<stdout>:  model_parallel_size ............. 8...........................updated
[1,0]<stdout>:  no_weight_tying ................. True........................updated
[1,0]<stdout>:  num_attention_heads ............. 96..........................updated
[1,0]<stdout>:  num_layers ...................... 4...........................updated
[1,0]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated
[1,0]<stdout>:  optimizer_type .................. Adam........................updated
[1,0]<stdout>:  output_layer_parallelism ........ column......................updated
[1,1]<stdout>:  num_nodes ....................... -1..........................default
[1,1]<stdout>:  num_samples ..................... 1...........................default
[1,1]<stdout>:  num_unique_layers ............... None........................default
[1,1]<stdout>:  num_workers ..................... 2...........................default
[1,1]<stdout>:  onnx_safe ....................... False.......................default
[1,1]<stdout>:  opt_pos_emb_offset .............. 0...........................default
[1,1]<stdout>:  output_layer_init_method ........ scaled_normal...............default[1,0]<stdout>:  partition_activations ........... True........................updated
[1,0]<stdout>:  pipe_parallel_size .............. 1...........................updated[1,6]<stdout>:
[1,6]<stdout>:  checkpoint_activations .......... True........................updated
[1,6]<stdout>:  checkpoint_factor ............... 1000........................updated
[1,6]<stdout>:  clip_grad ....................... 1.0.........................updated
[1,1]<stdout>:
[1,1]<stdout>:  override_lr_scheduler ........... False.......................default
[1,1]<stdout>:  padded_vocab_size ............... None........................default
[1,1]<stdout>:  param_sharing_style ............. grouped.....................default
[1,1]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,1]<stdout>:  prescale_gradients .............. False.......................default
[1,1]<stdout>:  profile_backward ................ False.......................default
[1,1]<stdout>:  prompt_end ...................... 
[1,1]<stdout>:...........................default
[1,1]<stdout>:  rank ............................ None........................default
[1,6]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,6]<stdout>:  data_impl ....................... mmap........................updated
[1,6]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,6]<stdout>:  deepspeed_mpi ................... True........................updated
[1,6]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,6]<stdout>:  eval_interval ................... 4000........................updated
[1,0]<stdout>:
[1,0]<stdout>:  pos_emb ......................... rotary......................updated
[1,0]<stdout>:  precision ....................... fp16........................updated
[1,0]<stdout>:  rotary_pct ...................... 0.25........................updated
[1,0]<stdout>:  save_iters ...................... []..........................updated
[1,0]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,0]<stdout>:  seq_length ...................... 4096........................updated
[1,0]<stdout>:  sparsity_config ................. {}..........................updated
[1,0]<stdout>:  synchronize_each_layer .......... True........................updated[1,1]<stdout>:  recompute ....................... False.......................default
[1,1]<stdout>:  return_logits ................... False.......................default
[1,1]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,1]<stdout>:  rotary_emb_base ................. 10000.......................default
[1,1]<stdout>:  rpe_max_distance ................ 128.........................default
[1,0]<stdout>:
[1,0]<stdout>:  text_gen_type ................... unconditional...............updated
[1,0]<stdout>:  train_batch_size ................ 16..........................updated
[1,0]<stdout>:  train_iters ..................... 3...........................updated
[1,0]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated[1,6]<stdout>:  eval_iters ...................... 10..........................updated
[1,6]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
[1,6]<stdout>:  gas ............................. 1...........................updated
[1,6]<stdout>:  global_num_gpus ................. 8...........................updated
[1,6]<stdout>:  gpt_j_residual .................. True........................updated
[1,6]<stdout>:  gradient_clipping ............... 1.0.........................updated
[1,6]<stdout>:  hidden_dropout .................. 0...........................updated
[1,1]<stdout>:  rpe_num_buckets ................. 32..........................default
[1,1]<stdout>:  sample_input_file ............... None........................default
[1,1]<stdout>:  sample_output_file .............. samples.txt.................default
[1,1]<stdout>:  save ............................ None........................default
[1,1]<stdout>:  save_base_shapes ................ False.......................default
[1,1]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,6]<stdout>:  hidden_size ..................... 12288.......................updated
[1,6]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated
[1,6]<stdout>:  log_interval .................... 10..........................updated
[1,6]<stdout>:  lr .............................. 0.0006......................updated
[1,6]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,6]<stdout>:  lr_decay_style .................. cosine......................updated
[1,6]<stdout>:  max_position_embeddings ......... 4096........................updated[1,0]<stdout>:
[1,0]<stdout>:  use_wandb ....................... False.......................updated
[1,0]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,0]<stdout>:  wall_clock_breakdown ............ True........................updated
[1,0]<stdout>:  weight_decay .................... 0.2.........................updated
[1,0]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,1]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,1]<stdout>:  scheduler ....................... None........................default
[1,1]<stdout>:  seed ............................ 1234........................default
[1,0]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,0]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
[1,0]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,0]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,6]<stdout>:
[1,6]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,6]<stdout>:  min_lr .......................... 6e-05.......................updated
[1,6]<stdout>:  model_parallel_size ............. 8...........................updated
[1,6]<stdout>:  no_weight_tying ................. True........................updated[1,1]<stdout>:  short_seq_prob .................. 0.1.........................default
[1,1]<stdout>:  soft_prompt_tuning .............. None........................default
[1,1]<stdout>:  sparse_gradients ................ False.......................default
[1,1]<stdout>:  split ........................... 969, 30, 1..................default
[1,1]<stdout>:  steps_per_print ................. 10..........................default
[1,6]<stdout>:
[1,6]<stdout>:  num_attention_heads ............. 96..........................updated
[1,6]<stdout>:  num_layers ...................... 4...........................updated
[1,6]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated
[1,6]<stdout>:  optimizer_type .................. Adam........................updated
[1,0]<stdout>:  zero_stage ...................... 3...........................updated
[1,0]<stdout>:  activation ...................... gelu........................default
[1,0]<stdout>:  adlr_autoresume ................. False.......................default
[1,0]<stdout>:  adlr_autoresume_interval ........ 1000........................default
[1,0]<stdout>:  amp ............................. None........................default[1,1]<stdout>:  temperature ..................... 0.0.........................default
[1,1]<stdout>:  tensorboard_dir ................. None........................default
[1,1]<stdout>:  test_data_paths ................. None........................default
[1,1]<stdout>:  test_data_weights ............... None........................default[1,0]<stdout>:
[1,0]<stdout>:  apply_query_key_layer_scaling ... False.......................default
[1,0]<stdout>:  attention_softmax_in_fp32 ....... False.......................default[1,6]<stdout>:  output_layer_parallelism ........ column......................updated
[1,6]<stdout>:  partition_activations ........... True........................updated
[1,6]<stdout>:  pipe_parallel_size .............. 1...........................updated
[1,6]<stdout>:  pos_emb ......................... rotary......................updated
[1,6]<stdout>:  precision ....................... fp16........................updated
[1,1]<stdout>:
[1,1]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default
[1,1]<stdout>:  top_k ........................... 0...........................default
[1,1]<stdout>:  top_p ........................... 0.0.........................default
[1,1]<stdout>:  train_data_paths ................ None........................default
[1,6]<stdout>:  rotary_pct ...................... 0.25........................updated
[1,6]<stdout>:  save_iters ...................... []..........................updated
[1,6]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,6]<stdout>:  seq_length ...................... 4096........................updated
[1,0]<stdout>:
[1,0]<stdout>:  autotuning ...................... None........................default
[1,0]<stdout>:  autotuning_run .................. None........................default
[1,0]<stdout>:  base_shapes_file ................ None........................default
[1,0]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,1]<stdout>:  train_data_weights .............. None........................default
[1,1]<stdout>:  use_bnb_optimizer ............... False.......................default
[1,1]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default
[1,0]<stdout>:  char_level_ppl .................. False.......................default
[1,0]<stdout>:  checkpoint_in_cpu ............... False.......................default
[1,0]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,0]<stdout>:  checkpoint_scale ................ linear......................default
[1,6]<stdout>:  sparsity_config ................. {}..........................updated
[1,6]<stdout>:  synchronize_each_layer .......... True........................updated
[1,6]<stdout>:  text_gen_type ................... unconditional...............updated
[1,6]<stdout>:  train_batch_size ................ 16..........................updated
[1,1]<stdout>:  use_cpu_initialization .......... False.......................default
[1,1]<stdout>:  use_mup ......................... False.......................default
[1,1]<stdout>:  use_shared_fs ................... True........................default
[1,1]<stdout>:  user_script ..................... None........................default
[1,1]<stdout>:  valid_data_paths ................ None........................default[1,6]<stdout>:  train_iters ..................... 3...........................updated
[1,6]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated
[1,6]<stdout>:  use_wandb ....................... False.......................updated
[1,0]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,0]<stdout>:  comment ......................... None........................default
[1,0]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,0]<stdout>:  coord_check ..................... False.......................default[1,1]<stdout>:
[1,1]<stdout>:  valid_data_weights .............. None........................default
[1,1]<stdout>:  wandb_group ..................... None........................default
[1,1]<stdout>:  wandb_host ...................... https://api.wandb.ai........default
[1,1]<stdout>:  wandb_init_all_ranks ............ False.......................default
[1,1]<stdout>:  wandb_project ................... neox........................default
[1,0]<stdout>:
[1,0]<stdout>:  curriculum_learning ............. None........................default
[1,0]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,0]<stdout>:  deepscale ....................... False.......................default
[1,0]<stdout>:  deepscale_config ................ None........................default[1,6]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,6]<stdout>:  wall_clock_breakdown ............ True........................updated
[1,6]<stdout>:  weight_decay .................... 0.2.........................updated
[1,6]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,6]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,6]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated[1,1]<stdout>:  wandb_team ...................... None........................default
[1,1]<stdout>:  warmup .......................... 0.01........................default
[1,1]<stdout>:  weight_by_num_documents ......... False.......................default[1,6]<stdout>:
[1,6]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,6]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,0]<stdout>:
[1,0]<stdout>:  deepspeed ....................... True........................default
[1,0]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,0]<stdout>:  deepspeed_slurm ................. False.......................default
[1,0]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,0]<stdout>:  distributed_backend ............. nccl........................default[1,1]<stdout>:
[1,1]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,1]<stdout>:  world_size ...................... None........................default
[1,1]<stdout>:  zero_allow_untested_optimizer ... False.......................default
[1,1]<stdout>:---------------- end of arguments ----------------
[1,0]<stdout>:
[1,0]<stdout>:  do_test ......................... None........................default
[1,0]<stdout>:  do_train ........................ None........................default
[1,0]<stdout>:  do_valid ........................ None........................default
[1,6]<stdout>:  zero_stage ...................... 3...........................updated
[1,6]<stdout>:  activation ...................... gelu........................default
[1,6]<stdout>:  adlr_autoresume ................. False.......................default
[1,6]<stdout>:  adlr_autoresume_interval ........ 1000........................default
[1,6]<stdout>:  amp ............................. None........................default
[1,6]<stdout>:  apply_query_key_layer_scaling ... False.......................default[1,0]<stdout>:  dump_state ...................... False.......................default
[1,0]<stdout>:  eod_mask_loss ................... False.......................default
[1,0]<stdout>:  eval_results_prefix ............. ............................default
[1,6]<stdout>:
[1,6]<stdout>:  attention_softmax_in_fp32 ....... False.......................default
[1,6]<stdout>:  autotuning ...................... None........................default
[1,6]<stdout>:  autotuning_run .................. None........................default
[1,0]<stdout>:  eval_tasks ...................... None........................default
[1,0]<stdout>:  exclude ......................... None........................default
[1,0]<stdout>:  exit_interval ................... None........................default[1,6]<stdout>:  base_shapes_file ................ None........................default
[1,6]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,6]<stdout>:  char_level_ppl .................. False.......................default
[1,0]<stdout>:
[1,0]<stdout>:  extra_save_iters ................ None........................default
[1,0]<stdout>:  finetune ........................ False.......................default
[1,0]<stdout>:  flops_profiler .................. None........................default
[1,6]<stdout>:  checkpoint_in_cpu ............... False.......................default
[1,6]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,6]<stdout>:  checkpoint_scale ................ linear......................default
[1,3]<stdout>:-------------------- arguments --------------------
[1,0]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,0]<stdout>:  fp32_allreduce .................. False.......................default
[1,0]<stdout>:  git_hash ........................ None........................default
[1,0]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,6]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,6]<stdout>:  comment ......................... None........................default
[1,6]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,6]<stdout>:  coord_check ..................... False.......................default[1,0]<stdout>:  gpt_j_tied ...................... False.......................default
[1,0]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,0]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default
[1,0]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default[1,6]<stdout>:
[1,6]<stdout>:  curriculum_learning ............. None........................default
[1,6]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,6]<stdout>:  deepscale ....................... False.......................default[1,0]<stdout>:
[1,0]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,0]<stdout>:  hysteresis ...................... 2...........................default
[1,0]<stdout>:  include ......................... None........................default[1,6]<stdout>:
[1,6]<stdout>:  deepscale_config ................ None........................default
[1,6]<stdout>:  deepspeed ....................... True........................default
[1,6]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,6]<stdout>:  deepspeed_slurm ................. False.......................default
[1,0]<stdout>:
[1,0]<stdout>:  init_method ..................... normal......................default
[1,0]<stdout>:  init_method_std ................. 0.02........................default
[1,0]<stdout>:  is_pipe_parallel ................ False.......................default
[1,0]<stdout>:  iteration ....................... None........................default[1,6]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,6]<stdout>:  distributed_backend ............. nccl........................default
[1,6]<stdout>:  do_test ......................... None........................default
[1,6]<stdout>:  do_train ........................ None........................default
[1,0]<stdout>:
[1,0]<stdout>:  keep_last_n_checkpoints ......... None........................default
[1,0]<stdout>:  launcher ........................ pdsh........................default
[1,0]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,0]<stdout>:  lazy_mpu_init ................... False.......................default[1,6]<stdout>:  do_valid ........................ None........................default
[1,6]<stdout>:  dump_state ...................... False.......................default
[1,6]<stdout>:  eod_mask_loss ................... False.......................default
[1,6]<stdout>:  eval_results_prefix ............. ............................default
[1,0]<stdout>:
[1,0]<stdout>:  load ............................ None........................default
[1,0]<stdout>:  local_rank ...................... None........................default
[1,0]<stdout>:  log_dir ......................... None........................default
[1,6]<stdout>:  eval_tasks ...................... None........................default
[1,6]<stdout>:  exclude ......................... None........................default
[1,6]<stdout>:  exit_interval ................... None........................default
[1,6]<stdout>:  extra_save_iters ................ None........................default
[1,0]<stdout>:  log_grad_norm ................... False.......................default
[1,0]<stdout>:  log_grad_pct_zeros .............. False.......................default
[1,0]<stdout>:  log_gradient_noise_scale ........ False.......................default
[1,0]<stdout>:  log_optimizer_states ............ False.......................default[1,6]<stdout>:  finetune ........................ False.......................default
[1,6]<stdout>:  flops_profiler .................. None........................default
[1,6]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,6]<stdout>:  fp32_allreduce .................. False.......................default
[1,0]<stdout>:
[1,0]<stdout>:  log_param_norm .................. False.......................default
[1,0]<stdout>:  loss_scale ...................... None........................default
[1,0]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,6]<stdout>:  git_hash ........................ None........................default
[1,6]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,6]<stdout>:  gpt_j_tied ...................... False.......................default
[1,0]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,0]<stdout>:  master_addr ..................... None........................default
[1,0]<stdout>:  master_port ..................... 29500.......................default
[1,0]<stdout>:  maximum_tokens .................. 64..........................default
[1,6]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,6]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default
[1,6]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default[1,0]<stdout>:  min_scale ....................... 1.0.........................default
[1,0]<stdout>:  mmap_warmup ..................... False.......................default
[1,0]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,6]<stdout>:
[1,6]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,6]<stdout>:  hysteresis ...................... 2...........................default
[1,6]<stdout>:  include ......................... None........................default[1,0]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,0]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,0]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,0]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default[1,6]<stdout>:
[1,6]<stdout>:  init_method ..................... normal......................default
[1,6]<stdout>:  init_method_std ................. 0.02........................default
[1,6]<stdout>:  is_pipe_parallel ................ False.......................default
[1,6]<stdout>:  iteration ....................... None........................default
[1,0]<stdout>:
[1,0]<stdout>:  mup_width_scale ................. 2...........................default
[1,0]<stdout>:  no_load_optim ................... False.......................default
[1,0]<stdout>:  no_load_rng ..................... False.......................default
[1,0]<stdout>:  no_save_optim ................... False.......................default
[1,6]<stdout>:  keep_last_n_checkpoints ......... None........................default
[1,6]<stdout>:  launcher ........................ pdsh........................default
[1,6]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,6]<stdout>:  lazy_mpu_init ................... False.......................default
[1,0]<stdout>:  no_save_rng ..................... False.......................default
[1,0]<stdout>:  no_ssh_check .................... False.......................default
[1,0]<stdout>:  norm ............................ layernorm...................default
[1,0]<stdout>:  num_gpus ........................ None........................default
[1,6]<stdout>:  load ............................ None........................default
[1,6]<stdout>:  local_rank ...................... None........................default
[1,6]<stdout>:  log_dir ......................... None........................default
[1,6]<stdout>:  log_grad_norm ................... False.......................default
[1,6]<stdout>:  log_grad_pct_zeros .............. False.......................default[1,0]<stdout>:  num_nodes ....................... -1..........................default
[1,0]<stdout>:  num_samples ..................... 1...........................default
[1,0]<stdout>:  num_unique_layers ............... None........................default
[1,0]<stdout>:  num_workers ..................... 2...........................default
[1,6]<stdout>:
[1,6]<stdout>:  log_gradient_noise_scale ........ False.......................default
[1,6]<stdout>:  log_optimizer_states ............ False.......................default
[1,6]<stdout>:  log_param_norm .................. False.......................default
[1,0]<stdout>:  onnx_safe ....................... False.......................default
[1,0]<stdout>:  opt_pos_emb_offset .............. 0...........................default
[1,0]<stdout>:  output_layer_init_method ........ scaled_normal...............default[1,6]<stdout>:  loss_scale ...................... None........................default
[1,6]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,6]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,0]<stdout>:
[1,0]<stdout>:  override_lr_scheduler ........... False.......................default
[1,0]<stdout>:  padded_vocab_size ............... None........................default
[1,0]<stdout>:  param_sharing_style ............. grouped.....................default
[1,0]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,6]<stdout>:  master_addr ..................... None........................default
[1,6]<stdout>:  master_port ..................... 29500.......................default
[1,6]<stdout>:  maximum_tokens .................. 64..........................default
[1,6]<stdout>:  min_scale ....................... 1.0.........................default
[1,6]<stdout>:  mmap_warmup ..................... False.......................default
[1,0]<stdout>:  prescale_gradients .............. False.......................default
[1,0]<stdout>:  profile_backward ................ False.......................default
[1,0]<stdout>:  prompt_end ...................... 
[1,0]<stdout>:...........................default
[1,0]<stdout>:  rank ............................ None........................default
[1,6]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,6]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,6]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,0]<stdout>:  recompute ....................... False.......................default
[1,0]<stdout>:  return_logits ................... False.......................default
[1,0]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,0]<stdout>:  rotary_emb_base ................. 10000.......................default[1,6]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,6]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default
[1,6]<stdout>:  mup_width_scale ................. 2...........................default
[1,6]<stdout>:  no_load_optim ................... False.......................default
[1,6]<stdout>:  no_load_rng ..................... False.......................default[1,0]<stdout>:
[1,0]<stdout>:  rpe_max_distance ................ 128.........................default
[1,0]<stdout>:  rpe_num_buckets ................. 32..........................default
[1,0]<stdout>:  sample_input_file ............... None........................default
[1,0]<stdout>:  sample_output_file .............. samples.txt.................default[1,3]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,3]<stdout>:  attention_dropout ............... 0...........................updated
[1,3]<stdout>:  batch_size ...................... 16..........................updated
[1,3]<stdout>:  bias_gelu_fusion ................ True........................updated[1,6]<stdout>:
[1,6]<stdout>:  no_save_optim ................... False.......................default
[1,6]<stdout>:  no_save_rng ..................... False.......................default
[1,6]<stdout>:  no_ssh_check .................... False.......................default
[1,6]<stdout>:  norm ............................ layernorm...................default[1,3]<stdout>:
[1,3]<stdout>:  checkpoint_activations .......... True........................updated
[1,3]<stdout>:  checkpoint_factor ............... 1000........................updated
[1,3]<stdout>:  clip_grad ....................... 1.0.........................updated
[1,0]<stdout>:
[1,0]<stdout>:  save ............................ None........................default
[1,0]<stdout>:  save_base_shapes ................ False.......................default
[1,0]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,0]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,0]<stdout>:  scheduler ....................... None........................default
[1,6]<stdout>:
[1,6]<stdout>:  num_gpus ........................ None........................default
[1,6]<stdout>:  num_nodes ....................... -1..........................default
[1,6]<stdout>:  num_samples ..................... 1...........................default
[1,6]<stdout>:  num_unique_layers ............... None........................default
[1,6]<stdout>:  num_workers ..................... 2...........................default
[1,0]<stdout>:  seed ............................ 1234........................default
[1,0]<stdout>:  short_seq_prob .................. 0.1.........................default
[1,0]<stdout>:  soft_prompt_tuning .............. None........................default
[1,0]<stdout>:  sparse_gradients ................ False.......................default
[1,0]<stdout>:  split ........................... 969, 30, 1..................default
[1,0]<stdout>:  steps_per_print ................. 10..........................default[1,3]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,3]<stdout>:  data_impl ....................... mmap........................updated
[1,3]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,3]<stdout>:  deepspeed_mpi ................... True........................updated
[1,3]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,3]<stdout>:  eval_interval ................... 4000........................updated
[1,6]<stdout>:  onnx_safe ....................... False.......................default
[1,6]<stdout>:  opt_pos_emb_offset .............. 0...........................default
[1,6]<stdout>:  output_layer_init_method ........ scaled_normal...............default
[1,6]<stdout>:  override_lr_scheduler ........... False.......................default
[1,6]<stdout>:  padded_vocab_size ............... None........................default
[1,6]<stdout>:  param_sharing_style ............. grouped.....................default
[1,6]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,6]<stdout>:  prescale_gradients .............. False.......................default
[1,3]<stdout>:  eval_iters ...................... 10..........................updated
[1,3]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
[1,3]<stdout>:  gas ............................. 1...........................updated
[1,3]<stdout>:  global_num_gpus ................. 8...........................updated
[1,3]<stdout>:  gpt_j_residual .................. True........................updated
[1,0]<stdout>:
[1,0]<stdout>:  temperature ..................... 0.0.........................default
[1,0]<stdout>:  tensorboard_dir ................. None........................default
[1,0]<stdout>:  test_data_paths ................. None........................default
[1,0]<stdout>:  test_data_weights ............... None........................default
[1,0]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default
[1,0]<stdout>:  top_k ........................... 0...........................default
[1,0]<stdout>:  top_p ........................... 0.0.........................default[1,6]<stdout>:  profile_backward ................ False.......................default
[1,6]<stdout>:  prompt_end ...................... 
[1,6]<stdout>:...........................default
[1,6]<stdout>:  rank ............................ None........................default
[1,6]<stdout>:  recompute ....................... False.......................default
[1,6]<stdout>:  return_logits ................... False.......................default[1,0]<stdout>:
[1,0]<stdout>:  train_data_paths ................ None........................default
[1,0]<stdout>:  train_data_weights .............. None........................default
[1,0]<stdout>:  use_bnb_optimizer ............... False.......................default
[1,3]<stdout>:  gradient_clipping ............... 1.0.........................updated
[1,3]<stdout>:  hidden_dropout .................. 0...........................updated
[1,3]<stdout>:  hidden_size ..................... 12288.......................updated
[1,3]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated
[1,3]<stdout>:  log_interval .................... 10..........................updated
[1,3]<stdout>:  lr .............................. 0.0006......................updated
[1,3]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,3]<stdout>:  lr_decay_style .................. cosine......................updated
[1,6]<stdout>:
[1,6]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,6]<stdout>:  rotary_emb_base ................. 10000.......................default
[1,6]<stdout>:  rpe_max_distance ................ 128.........................default
[1,6]<stdout>:  rpe_num_buckets ................. 32..........................default
[1,6]<stdout>:  sample_input_file ............... None........................default
[1,6]<stdout>:  sample_output_file .............. samples.txt.................default
[1,6]<stdout>:  save ............................ None........................default
[1,3]<stdout>:  max_position_embeddings ......... 4096........................updated
[1,3]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,3]<stdout>:  min_lr .......................... 6e-05.......................updated
[1,3]<stdout>:  model_parallel_size ............. 8...........................updated
[1,3]<stdout>:  no_weight_tying ................. True........................updated
[1,3]<stdout>:  num_attention_heads ............. 96..........................updated[1,0]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default
[1,0]<stdout>:  use_cpu_initialization .......... False.......................default
[1,0]<stdout>:  use_mup ......................... False.......................default
[1,0]<stdout>:  use_shared_fs ................... True........................default
[1,0]<stdout>:  user_script ..................... None........................default
[1,0]<stdout>:  valid_data_paths ................ None........................default
[1,0]<stdout>:  valid_data_weights .............. None........................default
[1,0]<stdout>:  wandb_group ..................... None........................default
[1,6]<stdout>:  save_base_shapes ................ False.......................default
[1,6]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,6]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,6]<stdout>:  scheduler ....................... None........................default
[1,6]<stdout>:  seed ............................ 1234........................default
[1,6]<stdout>:  short_seq_prob .................. 0.1.........................default
[1,0]<stdout>:  wandb_host ...................... https://api.wandb.ai........default
[1,0]<stdout>:  wandb_init_all_ranks ............ False.......................default
[1,0]<stdout>:  wandb_project ................... neox........................default
[1,0]<stdout>:  wandb_team ...................... None........................default
[1,0]<stdout>:  warmup .......................... 0.01........................default[1,3]<stdout>:
[1,3]<stdout>:  num_layers ...................... 4...........................updated
[1,3]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated
[1,3]<stdout>:  optimizer_type .................. Adam........................updated
[1,3]<stdout>:  output_layer_parallelism ........ column......................updated
[1,3]<stdout>:  partition_activations ........... True........................updated
[1,3]<stdout>:  pipe_parallel_size .............. 1...........................updated
[1,3]<stdout>:  pos_emb ......................... rotary......................updated
[1,6]<stdout>:  soft_prompt_tuning .............. None........................default
[1,6]<stdout>:  sparse_gradients ................ False.......................default
[1,6]<stdout>:  split ........................... 969, 30, 1..................default
[1,6]<stdout>:  steps_per_print ................. 10..........................default
[1,6]<stdout>:  temperature ..................... 0.0.........................default[1,3]<stdout>:  precision ....................... fp16........................updated
[1,3]<stdout>:  rotary_pct ...................... 0.25........................updated
[1,3]<stdout>:  save_iters ...................... []..........................updated
[1,3]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,3]<stdout>:  seq_length ...................... 4096........................updated
[1,0]<stdout>:
[1,0]<stdout>:  weight_by_num_documents ......... False.......................default
[1,0]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,0]<stdout>:  world_size ...................... None........................default
[1,0]<stdout>:  zero_allow_untested_optimizer ... False.......................default
[1,0]<stdout>:---------------- end of arguments ----------------
[1,6]<stdout>:
[1,6]<stdout>:  tensorboard_dir ................. None........................default
[1,6]<stdout>:  test_data_paths ................. None........................default
[1,6]<stdout>:  test_data_weights ............... None........................default
[1,6]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default
[1,6]<stdout>:  top_k ........................... 0...........................default
[1,3]<stdout>:  sparsity_config ................. {}..........................updated
[1,3]<stdout>:  synchronize_each_layer .......... True........................updated
[1,3]<stdout>:  text_gen_type ................... unconditional...............updated
[1,3]<stdout>:  train_batch_size ................ 16..........................updated
[1,3]<stdout>:  train_iters ..................... 3...........................updated
[1,3]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated
[1,3]<stdout>:  use_wandb ....................... False.......................updated
[1,3]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,6]<stdout>:  top_p ........................... 0.0.........................default
[1,6]<stdout>:  train_data_paths ................ None........................default
[1,6]<stdout>:  train_data_weights .............. None........................default
[1,6]<stdout>:  use_bnb_optimizer ............... False.......................default
[1,3]<stdout>:  wall_clock_breakdown ............ True........................updated
[1,3]<stdout>:  weight_decay .................... 0.2.........................updated
[1,3]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,3]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,3]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
[1,6]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default
[1,6]<stdout>:  use_cpu_initialization .......... False.......................default
[1,6]<stdout>:  use_mup ......................... False.......................default
[1,6]<stdout>:  use_shared_fs ................... True........................default
[1,3]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,3]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,3]<stdout>:  zero_stage ...................... 3...........................updated
[1,3]<stdout>:  activation ...................... gelu........................default
[1,3]<stdout>:  adlr_autoresume ................. False.......................default
[1,6]<stdout>:  user_script ..................... None........................default
[1,6]<stdout>:  valid_data_paths ................ None........................default
[1,6]<stdout>:  valid_data_weights .............. None........................default
[1,3]<stdout>:  adlr_autoresume_interval ........ 1000........................default
[1,3]<stdout>:  amp ............................. None........................default
[1,3]<stdout>:  apply_query_key_layer_scaling ... False.......................default
[1,3]<stdout>:  attention_softmax_in_fp32 ....... False.......................default
[1,6]<stdout>:  wandb_group ..................... None........................default
[1,6]<stdout>:  wandb_host ...................... https://api.wandb.ai........default
[1,6]<stdout>:  wandb_init_all_ranks ............ False.......................default
[1,6]<stdout>:  wandb_project ................... neox........................default
[1,3]<stdout>:  autotuning ...................... None........................default
[1,3]<stdout>:  autotuning_run .................. None........................default
[1,3]<stdout>:  base_shapes_file ................ None........................default
[1,3]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,6]<stdout>:  wandb_team ...................... None........................default
[1,6]<stdout>:  warmup .......................... 0.01........................default
[1,6]<stdout>:  weight_by_num_documents ......... False.......................default
[1,3]<stdout>:  char_level_ppl .................. False.......................default
[1,3]<stdout>:  checkpoint_in_cpu ............... False.......................default
[1,3]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,3]<stdout>:  checkpoint_scale ................ linear......................default
[1,6]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,6]<stdout>:  world_size ...................... None........................default
[1,6]<stdout>:  zero_allow_untested_optimizer ... False.......................default
[1,6]<stdout>:---------------- end of arguments ----------------
[1,3]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,3]<stdout>:  comment ......................... None........................default
[1,3]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,3]<stdout>:  coord_check ..................... False.......................default
[1,3]<stdout>:  curriculum_learning ............. None........................default
[1,3]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,3]<stdout>:  deepscale ....................... False.......................default
[1,3]<stdout>:  deepscale_config ................ None........................default
[1,3]<stdout>:  deepspeed ....................... True........................default
[1,3]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,3]<stdout>:  deepspeed_slurm ................. False.......................default
[1,3]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,3]<stdout>:  distributed_backend ............. nccl........................default
[1,3]<stdout>:  do_test ......................... None........................default
[1,3]<stdout>:  do_train ........................ None........................default
[1,3]<stdout>:  do_valid ........................ None........................default
[1,3]<stdout>:  dump_state ...................... False.......................default
[1,3]<stdout>:  eod_mask_loss ................... False.......................default
[1,3]<stdout>:  eval_results_prefix ............. ............................default
[1,3]<stdout>:  eval_tasks ...................... None........................default
[1,3]<stdout>:  exclude ......................... None........................default
[1,3]<stdout>:  exit_interval ................... None........................default
[1,3]<stdout>:  extra_save_iters ................ None........................default
[1,3]<stdout>:  finetune ........................ False.......................default
[1,3]<stdout>:  flops_profiler .................. None........................default
[1,3]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,3]<stdout>:  fp32_allreduce .................. False.......................default
[1,3]<stdout>:  git_hash ........................ None........................default
[1,3]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,3]<stdout>:  gpt_j_tied ...................... False.......................default
[1,3]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,3]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default
[1,3]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default
[1,3]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,3]<stdout>:  hysteresis ...................... 2...........................default
[1,3]<stdout>:  include ......................... None........................default
[1,3]<stdout>:  init_method ..................... normal......................default
[1,3]<stdout>:  init_method_std ................. 0.02........................default
[1,3]<stdout>:  is_pipe_parallel ................ False.......................default
[1,3]<stdout>:  iteration ....................... None........................default
[1,3]<stdout>:  keep_last_n_checkpoints ......... None........................default
[1,3]<stdout>:  launcher ........................ pdsh........................default
[1,3]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,3]<stdout>:  lazy_mpu_init ................... False.......................default
[1,3]<stdout>:  load ............................ None........................default
[1,3]<stdout>:  local_rank ...................... None........................default
[1,3]<stdout>:  log_dir ......................... None........................default
[1,3]<stdout>:  log_grad_norm ................... False.......................default
[1,3]<stdout>:  log_grad_pct_zeros .............. False.......................default
[1,3]<stdout>:  log_gradient_noise_scale ........ False.......................default
[1,3]<stdout>:  log_optimizer_states ............ False.......................default
[1,3]<stdout>:  log_param_norm .................. False.......................default
[1,3]<stdout>:  loss_scale ...................... None........................default
[1,3]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,3]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,3]<stdout>:  master_addr ..................... None........................default
[1,3]<stdout>:  master_port ..................... 29500.......................default
[1,3]<stdout>:  maximum_tokens .................. 64..........................default
[1,3]<stdout>:  min_scale ....................... 1.0.........................default
[1,3]<stdout>:  mmap_warmup ..................... False.......................default
[1,3]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,3]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,3]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,3]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,3]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default
[1,3]<stdout>:  mup_width_scale ................. 2...........................default
[1,3]<stdout>:  no_load_optim ................... False.......................default
[1,3]<stdout>:  no_load_rng ..................... False.......................default
[1,3]<stdout>:  no_save_optim ................... False.......................default
[1,3]<stdout>:  no_save_rng ..................... False.......................default
[1,3]<stdout>:  no_ssh_check .................... False.......................default
[1,3]<stdout>:  norm ............................ layernorm...................default
[1,3]<stdout>:  num_gpus ........................ None........................default
[1,3]<stdout>:  num_nodes ....................... -1..........................default
[1,3]<stdout>:  num_samples ..................... 1...........................default
[1,3]<stdout>:  num_unique_layers ............... None........................default
[1,3]<stdout>:  num_workers ..................... 2...........................default
[1,3]<stdout>:  onnx_safe ....................... False.......................default
[1,3]<stdout>:  opt_pos_emb_offset .............. 0...........................default
[1,3]<stdout>:  output_layer_init_method ........ scaled_normal...............default
[1,3]<stdout>:  override_lr_scheduler ........... False.......................default
[1,3]<stdout>:  padded_vocab_size ............... None........................default
[1,3]<stdout>:  param_sharing_style ............. grouped.....................default
[1,3]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,3]<stdout>:  prescale_gradients .............. False.......................default
[1,3]<stdout>:  profile_backward ................ False.......................default
[1,3]<stdout>:  prompt_end ...................... 
[1,3]<stdout>:...........................default
[1,3]<stdout>:  rank ............................ None........................default
[1,3]<stdout>:  recompute ....................... False.......................default
[1,3]<stdout>:  return_logits ................... False.......................default
[1,3]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,3]<stdout>:  rotary_emb_base ................. 10000.......................default
[1,3]<stdout>:  rpe_max_distance ................ 128.........................default
[1,3]<stdout>:  rpe_num_buckets ................. 32..........................default
[1,3]<stdout>:  sample_input_file ............... None........................default
[1,3]<stdout>:  sample_output_file .............. samples.txt.................default
[1,3]<stdout>:  save ............................ None........................default
[1,3]<stdout>:  save_base_shapes ................ False.......................default
[1,3]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,3]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,3]<stdout>:  scheduler ....................... None........................default
[1,3]<stdout>:  seed ............................ 1234........................default
[1,3]<stdout>:  short_seq_prob .................. 0.1.........................default
[1,3]<stdout>:  soft_prompt_tuning .............. None........................default
[1,3]<stdout>:  sparse_gradients ................ False.......................default
[1,3]<stdout>:  split ........................... 969, 30, 1..................default
[1,3]<stdout>:  steps_per_print ................. 10..........................default
[1,3]<stdout>:  temperature ..................... 0.0.........................default
[1,3]<stdout>:  tensorboard_dir ................. None........................default
[1,3]<stdout>:  test_data_paths ................. None........................default
[1,3]<stdout>:  test_data_weights ............... None........................default
[1,3]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default
[1,3]<stdout>:  top_k ........................... 0...........................default
[1,3]<stdout>:  top_p ........................... 0.0.........................default
[1,3]<stdout>:  train_data_paths ................ None........................default
[1,3]<stdout>:  train_data_weights .............. None........................default
[1,3]<stdout>:  use_bnb_optimizer ............... False.......................default
[1,3]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default
[1,3]<stdout>:  use_cpu_initialization .......... False.......................default
[1,3]<stdout>:  use_mup ......................... False.......................default
[1,3]<stdout>:  use_shared_fs ................... True........................default
[1,3]<stdout>:  user_script ..................... None........................default
[1,3]<stdout>:  valid_data_paths ................ None........................default
[1,3]<stdout>:  valid_data_weights .............. None........................default
[1,3]<stdout>:  wandb_group ..................... None........................default
[1,3]<stdout>:  wandb_host ...................... https://api.wandb.ai........default
[1,3]<stdout>:  wandb_init_all_ranks ............ False.......................default
[1,3]<stdout>:  wandb_project ................... neox........................default
[1,3]<stdout>:  wandb_team ...................... None........................default
[1,3]<stdout>:  warmup .......................... 0.01........................default
[1,3]<stdout>:  weight_by_num_documents ......... False.......................default
[1,3]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,3]<stdout>:  world_size ...................... None........................default
[1,3]<stdout>:  zero_allow_untested_optimizer ... False.......................default
[1,3]<stdout>:---------------- end of arguments ----------------
[1,2]<stdout>:-------------------- arguments --------------------
[1,2]<stdout>:  attention_config ................ ['flash', 'flash', 'flash', 'flash']updated
[1,2]<stdout>:  attention_dropout ............... 0...........................updated
[1,2]<stdout>:  batch_size ...................... 16..........................updated
[1,2]<stdout>:  bias_gelu_fusion ................ True........................updated
[1,2]<stdout>:  checkpoint_activations .......... True........................updated
[1,2]<stdout>:  checkpoint_factor ............... 1000........................updated
[1,2]<stdout>:  clip_grad ....................... 1.0.........................updated
[1,2]<stdout>:  config_files .................... {'sai_30B.yml': '{\n   "pipe-parallel-size": 1,\n   "model-parallel-size": 8,\n\n   "num-layers": 4,\n   "hidden-size": 12288,\n   "num-attention-heads": 96,\n   "seq-length": 4096,\n   "max-position-embeddings": 4096,\n   "norm": "layernorm",\n   "pos-emb": "rotary",\n   "rotary_pct": 0.25,\n   "no-weight-tying": true,\n   "gpt_j_residual": true,\n   "output_layer_parallelism": "column",\n\n   "attention-config": [[["flash"], 4]],\n\n   "scaled-upper-triang-masked-softmax-fusion": true,\n   "bias-gelu-fusion": true,\n\n   "optimizer": {\n     "type": "Adam",\n     "params": {\n       "lr": 0.0006,\n       "betas": [0.9, 0.95],\n       "eps": 1.0e-6\n     }\n   },\n   "min_lr": 0.00006,\n\n   "zero_optimization": {\n    "stage": 3,\n    "allgather_partitions": true,\n    #"allgather_bucket_size": 1260000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 126000000,\n    "stage3_max_live_parameters": 100000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n   "train_micro_batch_size_per_gpu": 16,\n   "gradient_accumulation_steps": 1,\n   "data-impl": "mmap",\n\n   "checkpoint-activations": true,\n   "checkpoint-num-layers": 1,\n   "partition-activations": true,\n   "synchronize-each-layer": true,\n\n   "gradient_clipping": 1.0,\n   "weight-decay": 0.2,\n   "hidden-dropout": 0,\n   "attention-dropout": 0,\n\n   "fp16": {\n     "fp16": true,\n     "enabled": true,\n     "loss_scale": 0,\n     "loss_scale_window": 1000,\n     "initial_scale_power": 12,\n     "hysteresis": 2,\n     "min_loss_scale": 1\n   },\n\n   "train-iters": 3,\n   "lr-decay-iters": 360000,\n   "distributed-backend": "nccl",\n   "lr-decay-style": "cosine",\n   "warmup": 0.01,\n   "checkpoint-factor": 1000,\n   "eval-interval": 4000,\n   "eval-iters": 10,\n\n   "log-interval": 10,\n   "steps_per_print": 10,\n   "wall_clock_breakdown": true,\n\n   "use_wandb": false,\n   "data-path": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document",\n   "vocab-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.json",\n   "merge-file": "/fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txt",\n   "deepspeed_mpi": true,\n   "hostfile": "/fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi"\n}\n'}updated
[1,2]<stdout>:  data_impl ....................... mmap........................updated
[1,2]<stdout>:  data_path ....................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_documentupdated
[1,2]<stdout>:  deepspeed_mpi ................... True........................updated
[1,2]<stdout>:  dynamic_loss_scale .............. True........................updated
[1,2]<stdout>:  eval_interval ................... 4000........................updated
[1,2]<stdout>:  eval_iters ...................... 10..........................updated
[1,2]<stdout>:  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
[1,2]<stdout>:  gas ............................. 1...........................updated
[1,2]<stdout>:  global_num_gpus ................. 8...........................updated
[1,2]<stdout>:  gpt_j_residual .................. True........................updated
[1,2]<stdout>:  gradient_clipping ............... 1.0.........................updated
[1,2]<stdout>:  hidden_dropout .................. 0...........................updated
[1,2]<stdout>:  hidden_size ..................... 12288.......................updated
[1,2]<stdout>:  hostfile ........................ /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpiupdated
[1,2]<stdout>:  log_interval .................... 10..........................updated
[1,2]<stdout>:  lr .............................. 0.0006......................updated
[1,2]<stdout>:  lr_decay_iters .................. 360000......................updated
[1,2]<stdout>:  lr_decay_style .................. cosine......................updated
[1,2]<stdout>:  max_position_embeddings ......... 4096........................updated
[1,2]<stdout>:  merge_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-merges.txtupdated
[1,2]<stdout>:  min_lr .......................... 6e-05.......................updated
[1,2]<stdout>:  model_parallel_size ............. 8...........................updated[1,2]<stdout>:
[1,2]<stdout>:  no_weight_tying ................. True........................updated
[1,2]<stdout>:  num_attention_heads ............. 96..........................updated
[1,2]<stdout>:  num_layers ...................... 4...........................updated
[1,2]<stdout>:  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}}updated
[1,2]<stdout>:  optimizer_type .................. Adam........................updated
[1,2]<stdout>:  output_layer_parallelism ........ column......................updated
[1,2]<stdout>:  partition_activations ........... True........................updated[1,2]<stdout>:
[1,2]<stdout>:  pipe_parallel_size .............. 1...........................updated
[1,2]<stdout>:  pos_emb ......................... rotary......................updated
[1,2]<stdout>:  precision ....................... fp16........................updated
[1,2]<stdout>:  rotary_pct ...................... 0.25........................updated
[1,2]<stdout>:  save_iters ...................... []..........................updated[1,2]<stdout>:
[1,2]<stdout>:  scaled_upper_triang_masked_softmax_fusion  True...............updated
[1,2]<stdout>:  seq_length ...................... 4096........................updated
[1,2]<stdout>:  sparsity_config ................. {}..........................updated[1,2]<stdout>:
[1,2]<stdout>:  synchronize_each_layer .......... True........................updated
[1,2]<stdout>:  text_gen_type ................... unconditional...............updated
[1,2]<stdout>:  train_batch_size ................ 16..........................updated
[1,2]<stdout>:  train_iters ..................... 3...........................updated
[1,2]<stdout>:  train_micro_batch_size_per_gpu .. 16..........................updated
[1,2]<stdout>:  use_wandb ....................... False.......................updated
[1,2]<stdout>:  vocab_file ...................... /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/gpt2-vocab.jsonupdated
[1,2]<stdout>:  wall_clock_breakdown ............ True........................updated
[1,2]<stdout>:  weight_decay .................... 0.2.........................updated
[1,2]<stdout>:  zero_allgather_bucket_size ...... 500000000...................updated
[1,2]<stdout>:  zero_contiguous_gradients ....... True........................updated
[1,2]<stdout>:  zero_optimization ............... {'stage': 3, 'allgather_partitions': True, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 126000000, 'stage3_max_live_parameters': 100000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
[1,2]<stdout>:  zero_reduce_bucket_size ......... 126000000...................updated
[1,2]<stdout>:  zero_reduce_scatter ............. True........................updated
[1,2]<stdout>:  zero_stage ...................... 3...........................updated
[1,2]<stdout>:  activation ...................... gelu........................default
[1,2]<stdout>:  adlr_autoresume ................. False.......................default
[1,2]<stdout>:  adlr_autoresume_interval ........ 1000........................default
[1,2]<stdout>:  amp ............................. None........................default
[1,2]<stdout>:  apply_query_key_layer_scaling ... False.......................default
[1,2]<stdout>:  attention_softmax_in_fp32 ....... False.......................default
[1,2]<stdout>:  autotuning ...................... None........................default
[1,2]<stdout>:  autotuning_run .................. None........................default
[1,2]<stdout>:  base_shapes_file ................ None........................default
[1,2]<stdout>:  bias_dropout_fusion ............. False.......................default
[1,2]<stdout>:  char_level_ppl .................. False.......................default
[1,2]<stdout>:  checkpoint_in_cpu ............... False.......................default
[1,2]<stdout>:  checkpoint_num_layers ........... 1...........................default
[1,2]<stdout>:  checkpoint_scale ................ linear......................default
[1,2]<stdout>:  checkpoint_validation_with_forward_pass  False................default
[1,2]<stdout>:  comment ......................... None........................default
[1,2]<stdout>:  contiguous_checkpointing ........ False.......................default
[1,2]<stdout>:  coord_check ..................... False.......................default
[1,2]<stdout>:  curriculum_learning ............. None........................default
[1,2]<stdout>:  curriculum_seqlen ............... 0...........................default
[1,2]<stdout>:  deepscale ....................... False.......................default
[1,2]<stdout>:  deepscale_config ................ None........................default
[1,2]<stdout>:  deepspeed ....................... True........................default
[1,2]<stdout>:  deepspeed_activation_checkpointing  True......................default
[1,2]<stdout>:  deepspeed_slurm ................. False.......................default
[1,2]<stdout>:  detect_nvlink_pairs ............. False.......................default
[1,2]<stdout>:  distributed_backend ............. nccl........................default
[1,2]<stdout>:  do_test ......................... None........................default
[1,2]<stdout>:  do_train ........................ None........................default
[1,2]<stdout>:  do_valid ........................ None........................default
[1,2]<stdout>:  dump_state ...................... False.......................default
[1,2]<stdout>:  eod_mask_loss ................... False.......................default
[1,2]<stdout>:  eval_results_prefix ............. ............................default
[1,2]<stdout>:  eval_tasks ...................... None........................default
[1,2]<stdout>:  exclude ......................... None........................default
[1,2]<stdout>:  exit_interval ................... None........................default
[1,2]<stdout>:  extra_save_iters ................ None........................default
[1,2]<stdout>:  finetune ........................ False.......................default
[1,2]<stdout>:  flops_profiler .................. None........................default
[1,2]<stdout>:  fp16_lm_cross_entropy ........... False.......................default
[1,2]<stdout>:  fp32_allreduce .................. False.......................default
[1,2]<stdout>:  git_hash ........................ None........................default
[1,2]<stdout>:  gmlp_attn_dim ................... 64..........................default
[1,2]<stdout>:  gpt_j_tied ...................... False.......................default
[1,2]<stdout>:  gradient_accumulation_steps ..... 1...........................default
[1,2]<stdout>:  gradient_noise_scale_cpu_offload  False.......................default
[1,2]<stdout>:  gradient_noise_scale_n_batches .. 5...........................default
[1,2]<stdout>:  gradient_predivide_factor ....... 1.0.........................default
[1,2]<stdout>:  hysteresis ...................... 2...........................default
[1,2]<stdout>:  include ......................... None........................default
[1,2]<stdout>:  init_method ..................... normal......................default
[1,2]<stdout>:  init_method_std ................. 0.02........................default
[1,2]<stdout>:  is_pipe_parallel ................ False.......................default
[1,2]<stdout>:  iteration ....................... None........................default
[1,2]<stdout>:  keep_last_n_checkpoints ......... None........................default
[1,2]<stdout>:  launcher ........................ pdsh........................default
[1,2]<stdout>:  layernorm_epsilon ............... 1e-05.......................default
[1,2]<stdout>:  lazy_mpu_init ................... False.......................default
[1,2]<stdout>:  load ............................ None........................default
[1,2]<stdout>:  local_rank ...................... None........................default
[1,2]<stdout>:  log_dir ......................... None........................default
[1,2]<stdout>:  log_grad_norm ................... False.......................default
[1,2]<stdout>:  log_grad_pct_zeros .............. False.......................default
[1,2]<stdout>:  log_gradient_noise_scale ........ False.......................default
[1,2]<stdout>:  log_optimizer_states ............ False.......................default
[1,2]<stdout>:  log_param_norm .................. False.......................default
[1,2]<stdout>:  loss_scale ...................... None........................default[1,2]<stdout>:
[1,2]<stdout>:  loss_scale_window ............... 1000.0......................default
[1,2]<stdout>:  make_vocab_size_divisible_by .... 128.........................default
[1,2]<stdout>:  master_addr ..................... None........................default
[1,2]<stdout>:  master_port ..................... 29500.......................default
[1,2]<stdout>:  maximum_tokens .................. 64..........................default[1,2]<stdout>:
[1,2]<stdout>:  min_scale ....................... 1.0.........................default
[1,2]<stdout>:  mmap_warmup ..................... False.......................default
[1,2]<stdout>:  mup_attn_temp ................... 1.0.........................default
[1,2]<stdout>:  mup_embedding_mult .............. 1.0.........................default
[1,2]<stdout>:  mup_init_scale .................. 1.0.........................default
[1,2]<stdout>:  mup_output_temp ................. 1.0.........................default
[1,2]<stdout>:  mup_rp_embedding_mult ........... 1.0.........................default
[1,2]<stdout>:  mup_width_scale ................. 2...........................default
[1,2]<stdout>:  no_load_optim ................... False.......................default
[1,2]<stdout>:  no_load_rng ..................... False.......................default
[1,2]<stdout>:  no_save_optim ................... False.......................default
[1,2]<stdout>:  no_save_rng ..................... False.......................default
[1,2]<stdout>:  no_ssh_check .................... False.......................default
[1,2]<stdout>:  norm ............................ layernorm...................default
[1,2]<stdout>:  num_gpus ........................ None........................default
[1,2]<stdout>:  num_nodes ....................... -1..........................default
[1,2]<stdout>:  num_samples ..................... 1...........................default
[1,2]<stdout>:  num_unique_layers ............... None........................default
[1,2]<stdout>:  num_workers ..................... 2...........................default
[1,2]<stdout>:  onnx_safe ....................... False.......................default
[1,2]<stdout>:  opt_pos_emb_offset .............. 0...........................default
[1,2]<stdout>:  output_layer_init_method ........ scaled_normal...............default
[1,2]<stdout>:  override_lr_scheduler ........... False.......................default
[1,2]<stdout>:  padded_vocab_size ............... None........................default
[1,2]<stdout>:  param_sharing_style ............. grouped.....................default
[1,2]<stdout>:  pipe_partition_method ........... type:transformer|mlp........default
[1,2]<stdout>:  prescale_gradients .............. False.......................default
[1,2]<stdout>:  profile_backward ................ False.......................default
[1,2]<stdout>:  prompt_end ...................... 
[1,2]<stdout>:...........................default
[1,2]<stdout>:  rank ............................ None........................default
[1,2]<stdout>:  recompute ....................... False.......................default
[1,2]<stdout>:  return_logits ................... False.......................default
[1,2]<stdout>:  rms_norm_epsilon ................ 1e-08.......................default
[1,2]<stdout>:  rotary_emb_base ................. 10000.......................default
[1,2]<stdout>:  rpe_max_distance ................ 128.........................default
[1,2]<stdout>:  rpe_num_buckets ................. 32..........................default
[1,2]<stdout>:  sample_input_file ............... None........................default
[1,2]<stdout>:  sample_output_file .............. samples.txt.................default
[1,2]<stdout>:  save ............................ None........................default
[1,2]<stdout>:  save_base_shapes ................ False.......................default
[1,2]<stdout>:  scaled_masked_softmax_fusion .... False.......................default
[1,2]<stdout>:  scalenorm_epsilon ............... 1e-08.......................default
[1,2]<stdout>:  scheduler ....................... None........................default
[1,2]<stdout>:  seed ............................ 1234........................default
[1,2]<stdout>:  short_seq_prob .................. 0.1.........................default
[1,2]<stdout>:  soft_prompt_tuning .............. None........................default
[1,2]<stdout>:  sparse_gradients ................ False.......................default
[1,2]<stdout>:  split ........................... 969, 30, 1..................default
[1,2]<stdout>:  steps_per_print ................. 10..........................default
[1,2]<stdout>:  temperature ..................... 0.0.........................default
[1,2]<stdout>:  tensorboard_dir ................. None........................default
[1,2]<stdout>:  test_data_paths ................. None........................default
[1,2]<stdout>:  test_data_weights ............... None........................default
[1,2]<stdout>:  tokenizer_type .................. GPT2BPETokenizer............default
[1,2]<stdout>:  top_k ........................... 0...........................default
[1,2]<stdout>:  top_p ........................... 0.0.........................default
[1,2]<stdout>:  train_data_paths ................ None........................default
[1,2]<stdout>:  train_data_weights .............. None........................default
[1,2]<stdout>:  use_bnb_optimizer ............... False.......................default
[1,2]<stdout>:  use_checkpoint_lr_scheduler ..... False.......................default
[1,2]<stdout>:  use_cpu_initialization .......... False.......................default
[1,2]<stdout>:  use_mup ......................... False.......................default
[1,2]<stdout>:  use_shared_fs ................... True........................default
[1,2]<stdout>:  user_script ..................... None........................default
[1,2]<stdout>:  valid_data_paths ................ None........................default
[1,2]<stdout>:  valid_data_weights .............. None........................default
[1,2]<stdout>:  wandb_group ..................... None........................default
[1,2]<stdout>:  wandb_host ...................... https://api.wandb.ai........default
[1,2]<stdout>:  wandb_init_all_ranks ............ False.......................default
[1,2]<stdout>:  wandb_project ................... neox........................default
[1,2]<stdout>:  wandb_team ...................... None........................default
[1,2]<stdout>:  warmup .......................... 0.01........................default
[1,2]<stdout>:  weight_by_num_documents ......... False.......................default
[1,2]<stdout>:  weighted_sampler_alpha .......... 0.3.........................default
[1,2]<stdout>:  world_size ...................... None........................default
[1,2]<stdout>:  zero_allow_untested_optimizer ... False.......................default
[1,2]<stdout>:---------------- end of arguments ----------------
[1,3]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,1]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,5]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=5, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,4]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=4, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,6]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=6, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,2]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,7]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=7, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,0]<stdout>:[2023-03-23 19:28:23,684] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.10.18.134, master_port=29500
[1,0]<stdout>:NeoXArgs.configure_distributed_args() using world size: 8 and model-parallel size: 8 
[1,0]<stdout>:> building GPT2BPETokenizer tokenizer ...
[1,0]<stdout>: > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
[1,0]<stdout>:> initializing torch distributed ...
[1,0]<stdout>:[2023-03-23 19:28:25,392] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1,0]<stdout>:> initializing model parallel with size 8
[1,0]<stdout>:MPU DP: [0]
[1,0]<stdout>:MPU DP: [1]
[1,0]<stdout>:MPU DP: [2]
[1,0]<stdout>:MPU DP: [3]
[1,0]<stdout>:MPU DP: [4]
[1,0]<stdout>:MPU DP: [5]
[1,0]<stdout>:MPU DP: [6]
[1,0]<stdout>:MPU DP: [7]
[1,0]<stdout>:MPU PP: [0]
[1,0]<stdout>:MPU PP: [1]
[1,0]<stdout>:MPU PP: [2]
[1,0]<stdout>:MPU PP: [3]
[1,0]<stdout>:MPU PP: [4]
[1,0]<stdout>:MPU PP: [5]
[1,0]<stdout>:MPU PP: [6]
[1,0]<stdout>:MPU PP: [7]
[1,0]<stdout>:MPU MP: [0, 1, 2, 3, 4, 5, 6, 7]
[1,0]<stdout>:> setting random seeds to 1234 ...
[1,0]<stdout>:[2023-03-23 19:28:26,145] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[1,0]<stdout>:make: Entering directory '/fsx/sliuxl/EleutherAI/gpt-neox/megatron/data'
[1,0]<stdout>:make: Nothing to be done for 'default'.
[1,0]<stdout>:make: Leaving directory '/fsx/sliuxl/EleutherAI/gpt-neox/megatron/data'
[1,0]<stdout>:building GPT2 model ...
[1,0]<stdout>:SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
[1,0]<stdout>:Using topology: [1,0]<stdout>:{ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=0, model=2): 2, ProcessCoord(pipe=0, data=0, model=3): 3, ProcessCoord(pipe=0, data=0, model=4): 4, ProcessCoord(pipe=0, data=0, model=5): 5, ProcessCoord(pipe=0, data=0, model=6): 6, ProcessCoord(pipe=0, data=0, model=7): 7}
[1,0]<stdout>:[2023-03-23 19:28:27,869] [INFO] [module.py:372:_partition_layers] Partitioning pipeline stages with method type:transformer|mlp
[1,0]<stdout>:stage=0 layers=9
[1,0]<stdout>:     0: EmbeddingPipe
[1,0]<stdout>:     1: _pre_transformer_block
[1,0]<stdout>:     2: ParallelTransformerLayerPipe
[1,0]<stdout>:     3: ParallelTransformerLayerPipe
[1,0]<stdout>:     4: ParallelTransformerLayerPipe
[1,0]<stdout>:     5: ParallelTransformerLayerPipe
[1,0]<stdout>:     6: _post_transformer_block
[1,0]<stdout>:     7: NormPipe
[1,0]<stdout>:     8: ParallelLinearPipe
[1,0]<stdout>:  loss: partial
[1,2]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,4]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,1]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,3]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,5]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,7]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,6]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,0]<stdout>:Configuring Optimizer type: Adam with params: {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}
[1,0]<stdout>:WARNING: APEX not installed - defaulting to deepspeed's fused adam
[1,2]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,4]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,1]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,6]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,3]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,5]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,0]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,7]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,2]<stderr>:Detected CUDA files, patching ldflags
[1,2]<stderr>:Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...
[1,2]<stderr>:Building extension module fused_adam...
[1,2]<stderr>:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1,2]<stdout>:ninja: no work to do.
[1,2]<stderr>:Loading extension module fused_adam...
[1,2]<stdout>:Time to load fused_adam op: 0.22799110412597656 seconds
[1,2]<stdout>:[2023-03-23 19:28:28,337] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,6]<stderr>:Loading extension module fused_adam...
[1,6]<stdout>:Time to load fused_adam op: 0.20325875282287598 seconds
[1,3]<stderr>:Loading extension module fused_adam...
[1,5]<stderr>:Loading extension module fused_adam...
[1,3]<stdout>:Time to load fused_adam op: 0.20314311981201172 seconds
[1,5]<stdout>:Time to load fused_adam op: 0.20291781425476074 seconds
[1,0]<stderr>:Loading extension module fused_adam...
[1,6]<stdout>:[2023-03-23 19:28:28,341] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,7]<stderr>:Loading extension module fused_adam...
[1,0]<stdout>:Time to load fused_adam op: 0.20292043685913086 seconds
[1,2]<stdout>:compute-st-worker-41:16538:16538 [2] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,7]<stdout>:Time to load fused_adam op: 0.20284461975097656 seconds
[1,3]<stdout>:[2023-03-23 19:28:28,343] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,5]<stdout>:[2023-03-23 19:28:28,343] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,0]<stdout>:> learning rate decay style: cosine
[1,0]<stdout>:DeepSpeed is enabled.
[1,2]<stdout>:compute-st-worker-41:16538:16538 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,2]<stdout>:compute-st-worker-41:16538:16538 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,2]<stdout>:compute-st-worker-41:16538:16538 [2] NCCL INFO cudaDriverVersion 11070
[1,2]<stdout>:NCCL version 2.14.3+cuda11.7
[1,0]<stdout>:[2023-03-23 19:28:28,343] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed info: version=0.8.3+457850d, git-hash=457850d, git-branch=HEAD
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,0]<stdout>:[2023-03-23 19:28:28,345] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,7]<stdout>:[2023-03-23 19:28:28,345] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,6]<stdout>:compute-st-worker-41:16542:16542 [6] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,6]<stdout>:compute-st-worker-41:16542:16542 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,6]<stdout>:compute-st-worker-41:16542:16542 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,6]<stdout>:compute-st-worker-41:16542:16542 [6] NCCL INFO cudaDriverVersion 11070
[1,6]<stdout>:NCCL version 2.14.3+cuda11.7
[1,3]<stdout>:compute-st-worker-41:16539:16539 [3] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,5]<stdout>:compute-st-worker-41:16541:16541 [5] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,3]<stdout>:compute-st-worker-41:16539:16539 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,3]<stdout>:compute-st-worker-41:16539:16539 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,3]<stdout>:compute-st-worker-41:16539:16539 [3] NCCL INFO cudaDriverVersion 11070
[1,3]<stdout>:NCCL version 2.14.3+cuda11.7
[1,5]<stdout>:compute-st-worker-41:16541:16541 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,5]<stdout>:compute-st-worker-41:16541:16541 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,5]<stdout>:compute-st-worker-41:16541:16541 [5] NCCL INFO cudaDriverVersion 11070
[1,5]<stdout>:NCCL version 2.14.3+cuda11.7
[1,0]<stdout>:compute-st-worker-41:16536:16536 [0] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,0]<stdout>:compute-st-worker-41:16536:16536 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,0]<stdout>:compute-st-worker-41:16536:16536 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO NET/OFI Selected Provider is efa
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Using network AWS Libfabric
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,7]<stdout>:compute-st-worker-41:16543:16543 [7] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,0]<stdout>:compute-st-worker-41:16536:16536 [0] NCCL INFO cudaDriverVersion 11070
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,0]<stdout>:NCCL version 2.14.3+cuda11.7
[1,7]<stdout>:compute-st-worker-41:16543:16543 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,7]<stdout>:compute-st-worker-41:16543:16543 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,7]<stdout>:compute-st-worker-41:16543:16543 [7] NCCL INFO cudaDriverVersion 11070
[1,7]<stdout>:NCCL version 2.14.3+cuda11.7
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO NET/OFI Selected Provider is efa
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Using network AWS Libfabric
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO NET/OFI Selected Provider is efa
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Using network AWS Libfabric
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO NET/OFI Selected Provider is efa
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Using network AWS Libfabric
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO NET/OFI Selected Provider is efa
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Using network AWS Libfabric
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO NET/OFI Selected Provider is efa
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Using network AWS Libfabric
[1,4]<stderr>:Loading extension module fused_adam...
[1,1]<stderr>:Loading extension module fused_adam...
[1,4]<stdout>:Time to load fused_adam op: 0.3036072254180908 seconds
[1,1]<stdout>:Time to load fused_adam op: 0.30374979972839355 seconds
[1,4]<stdout>:[2023-03-23 19:28:28,432] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,1]<stdout>:[2023-03-23 19:28:28,433] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,4]<stdout>:compute-st-worker-41:16540:16540 [4] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,4]<stdout>:compute-st-worker-41:16540:16540 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,4]<stdout>:compute-st-worker-41:16540:16540 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,4]<stdout>:compute-st-worker-41:16540:16540 [4] NCCL INFO cudaDriverVersion 11070
[1,4]<stdout>:NCCL version 2.14.3+cuda11.7
[1,1]<stdout>:compute-st-worker-41:16537:16537 [1] NCCL INFO Bootstrap : Using ens32:10.10.18.134<0>
[1,1]<stdout>:compute-st-worker-41:16537:16537 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
[1,1]<stdout>:compute-st-worker-41:16537:16537 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).
[1,1]<stdout>:compute-st-worker-41:16537:16537 [1] NCCL INFO cudaDriverVersion 11070
[1,1]<stdout>:NCCL version 2.14.3+cuda11.7
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.4.0aws
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 00/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 01/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 02/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 03/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 04/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 05/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 06/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 07/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 08/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 09/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 10/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 11/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 12/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 13/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 14/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 15/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 16/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 17/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 18/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 19/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 20/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 21/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 22/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 23/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 24/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 25/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 26/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 27/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 28/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 29/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO NET/OFI Selected Provider is efa
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Using network AWS Libfabric
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 30/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Channel 31/32 :    0
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO NET/OFI Selected Provider is efa
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Using network AWS Libfabric
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Connected all rings
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO Connected all trees
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,2]<stdout>:compute-st-worker-41:16538:18179 [2] NCCL INFO comm 0x562fc780c2e0 rank 0 nranks 1 cudaDev 2 busId 201c0 - Init COMPLETE
[1,2]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 00/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 01/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 02/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 03/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 04/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 05/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 06/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 07/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 08/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 09/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 10/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 11/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 12/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 13/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 14/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 15/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 16/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 17/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 18/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 19/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 20/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 21/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 22/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 23/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 24/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 25/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 26/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 27/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 28/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 29/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 30/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Channel 31/32 :    0
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Connected all rings
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO Connected all trees
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,6]<stdout>:compute-st-worker-41:16542:18181 [6] NCCL INFO comm 0x55876b818dc0 rank 0 nranks 1 cudaDev 6 busId a01c0 - Init COMPLETE
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 00/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 01/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 02/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 03/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 04/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 05/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 06/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 07/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 08/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 09/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 10/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 11/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 12/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 13/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 14/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 15/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 16/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 17/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 18/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 19/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 20/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 21/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 22/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 23/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 24/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 25/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 26/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 27/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 28/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 29/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 30/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Channel 31/32 :    0
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,6]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 00/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 01/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 02/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 03/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 04/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 05/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 06/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 07/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 08/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 09/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 10/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 11/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 12/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 13/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 14/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 15/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 16/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 17/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 18/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 19/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 20/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 21/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 22/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 23/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 24/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 25/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 26/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 27/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 28/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 29/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 30/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Channel 31/32 :    0
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Connected all rings
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO Connected all trees
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Connected all rings
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO Connected all trees
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 00/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 01/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 02/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 03/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 04/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 05/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 06/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 07/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 08/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 09/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 10/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 11/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 12/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 13/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 14/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 15/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 16/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 17/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 18/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 19/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 20/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 21/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 22/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 23/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 24/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 25/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 26/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 27/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 28/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 29/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 30/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Channel 31/32 :    0
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 00/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 01/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 02/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 03/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 04/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 05/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 06/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 07/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 08/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 09/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 10/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 11/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 12/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 13/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 14/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 15/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 16/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 17/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 18/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 19/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 20/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 21/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 22/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 23/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 24/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 25/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 26/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 27/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 28/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 29/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 30/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Channel 31/32 :    0
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Connected all rings
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO Connected all trees
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Connected all rings
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO Connected all trees
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,3]<stdout>:compute-st-worker-41:16539:18184 [3] NCCL INFO comm 0x55c92572d240 rank 0 nranks 1 cudaDev 3 busId 201d0 - Init COMPLETE
[1,5]<stdout>:compute-st-worker-41:16541:18186 [5] NCCL INFO comm 0x55ab1ef46bc0 rank 0 nranks 1 cudaDev 5 busId 901d0 - Init COMPLETE
[1,0]<stdout>:compute-st-worker-41:16536:18188 [0] NCCL INFO comm 0x557e560de990 rank 0 nranks 1 cudaDev 0 busId 101c0 - Init COMPLETE
[1,3]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,7]<stdout>:compute-st-worker-41:16543:18189 [7] NCCL INFO comm 0x55a4eac99fc0 rank 0 nranks 1 cudaDev 7 busId a01d0 - Init COMPLETE
[1,0]<stdout>:[2023-03-23 19:28:28,822] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[1,0]<stdout>:[2023-03-23 19:28:28,823] [INFO] [logging.py:77:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[1,0]<stdout>:[2023-03-23 19:28:28,823] [INFO] [logging.py:77:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[1,5]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,0]<stdout>:[2023-03-23 19:28:28,823] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[1,0]<stdout>:[2023-03-23 19:28:28,823] [INFO] [utils.py:55:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[1,0]<stdout>:[2023-03-23 19:28:28,823] [INFO] [logging.py:77:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[1,7]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,2]<stderr>:Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/utils/build.ninja...
[1,2]<stderr>:Building extension module utils...
[1,2]<stderr>:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 00/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 01/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 02/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 03/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 04/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 05/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 06/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 07/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 08/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 09/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 10/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 11/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 12/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 13/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 14/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 15/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 16/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 17/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 18/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 19/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 20/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 21/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 22/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 23/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 24/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 25/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 26/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 27/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 28/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 29/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 30/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Channel 31/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 00/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 01/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 02/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 03/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 04/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 05/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 06/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 07/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 08/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 09/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 10/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 11/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 12/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 13/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 14/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 15/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 16/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 17/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 18/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 19/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 20/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 21/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 22/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 23/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 24/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 25/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Connected all rings
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 26/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 27/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 28/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 29/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 30/32 :    0
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Channel 31/32 :    0
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO Connected all trees
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Connected all rings
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO Connected all trees
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,4]<stdout>:compute-st-worker-41:16540:18192 [4] NCCL INFO comm 0x556dfe2a8340 rank 0 nranks 1 cudaDev 4 busId 901c0 - Init COMPLETE
[1,1]<stdout>:compute-st-worker-41:16537:18193 [1] NCCL INFO comm 0x55aa0fc3c250 rank 0 nranks 1 cudaDev 1 busId 101d0 - Init COMPLETE
[1,4]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,1]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,2]<stdout>:ninja: no work to do.
[1,0]<stdout>:[2023-03-23 19:28:28,963] [INFO] [utils.py:829:see_memory_usage] Stage 3 initialize beginning
[1,0]<stdout>:[2023-03-23 19:28:28,963] [INFO] [utils.py:830:see_memory_usage] MA 1.98 GB         Max_MA 3.96 GB         CA 3.96 GB         Max_CA 4 GB 
[1,0]<stdout>:[2023-03-23 19:28:28,963] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.1 GB, percent = 4.1%
[1,0]<stdout>:[2023-03-23 19:28:28,964] [INFO] [stage3.py:113:__init__] Reduce bucket size 126000000
[1,0]<stdout>:[2023-03-23 19:28:28,964] [INFO] [stage3.py:114:__init__] Prefetch bucket size 50,000,000
[1,0]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,2]<stderr>:Loading extension module utils...
[1,2]<stdout>:Time to load utils op: 0.2810497283935547 seconds
[1,2]<stdout>:[2023-03-23 19:28:28,970] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,6]<stderr>:Loading extension module utils...
[1,6]<stdout>:Time to load utils op: 0.20275282859802246 seconds
[1,6]<stdout>:[2023-03-23 19:28:28,975] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,3]<stderr>:Loading extension module utils...
[1,5]<stderr>:Loading extension module utils...
[1,3]<stdout>:Time to load utils op: 0.2029111385345459 seconds
[1,3]<stdout>:[2023-03-23 19:28:29,025] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,5]<stdout>:Time to load utils op: 0.20267462730407715 seconds
[1,5]<stdout>:[2023-03-23 19:28:29,027] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,4]<stderr>:Loading extension module utils...
[1,7]<stderr>:Loading extension module utils...
[1,1]<stderr>:Loading extension module utils...
[1,4]<stdout>:Time to load utils op: 0.10246396064758301 seconds
[1,7]<stdout>:Time to load utils op: 0.20273590087890625 seconds
[1,4]<stdout>:[2023-03-23 19:28:29,035] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,1]<stdout>:Time to load utils op: 0.10254955291748047 seconds
[1,7]<stdout>:[2023-03-23 19:28:29,036] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,1]<stdout>:[2023-03-23 19:28:29,037] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,0]<stderr>:Loading extension module utils...
[1,0]<stdout>:Time to load utils op: 0.10279297828674316 seconds
[1,0]<stdout>:[2023-03-23 19:28:29,149] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[1,0]<stdout>:[2023-03-23 19:28:29,150] [INFO] [utils.py:830:see_memory_usage] MA 1.98 GB         Max_MA 1.98 GB         CA 3.96 GB         Max_CA 4 GB 
[1,0]<stdout>:[2023-03-23 19:28:29,150] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.1 GB, percent = 4.1%
[1,0]<stdout>:[2023-03-23 19:28:29,151] [WARNING] [config_utils.py:75:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead
[1,0]<stdout>:Parameter Offload: Total persistent parameters: 362496 in 34 params
[1,0]<stdout>:[2023-03-23 19:28:29,226] [INFO] [utils.py:829:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[1,0]<stdout>:[2023-03-23 19:28:29,227] [INFO] [utils.py:830:see_memory_usage] MA 1.98 GB         Max_MA 2.13 GB         CA 3.96 GB         Max_CA 4 GB 
[1,0]<stdout>:[2023-03-23 19:28:29,227] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.1 GB, percent = 4.1%
[1,0]<stdout>:[2023-03-23 19:28:29,293] [INFO] [utils.py:829:see_memory_usage] Before creating fp16 partitions
[1,0]<stdout>:[2023-03-23 19:28:29,293] [INFO] [utils.py:830:see_memory_usage] MA 1.98 GB         Max_MA 1.98 GB         CA 3.96 GB         Max_CA 4 GB 
[1,0]<stdout>:[2023-03-23 19:28:29,293] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.1 GB, percent = 4.1%
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Using network AWS Libfabric
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Using network AWS Libfabric
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Using network AWS Libfabric
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Using network AWS Libfabric
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Using network AWS Libfabric
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Using network AWS Libfabric
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Using network AWS Libfabric
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Using network AWS Libfabric
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 00/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 03/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 04/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 07/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 08/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 08/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 08/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 08/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 08/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 09/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 09/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 09/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 09/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 09/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 10/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 10/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 10/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 10/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 11/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 10/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 11/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 11/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 11/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 12/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 12/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 12/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 11/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 12/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 13/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 13/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 13/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 12/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 13/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 14/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 14/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 14/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 14/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 13/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 15/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 15/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 15/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 15/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 14/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 16/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 16/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 16/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 16/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 15/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 17/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 17/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 17/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 17/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 16/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 18/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 18/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 18/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 18/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 17/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 19/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 19/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 19/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 19/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 18/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 20/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 20/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 20/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 20/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 19/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 21/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 21/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 21/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 21/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 20/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 22/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 22/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 22/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 22/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 21/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Channel 23/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 23/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 23/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 23/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 22/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 23/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Connected all rings
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Connected all rings
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Connected all rings
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Connected all rings
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Connected all rings
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Connected all rings
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Connected all rings
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 00/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Connected all rings
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 04/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 08/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 09/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 10/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 11/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 12/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 13/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 14/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 15/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 16/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 17/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 18/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 19/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 20/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 21/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 22/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Channel 23/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 03/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 07/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 08/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 08/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 08/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 09/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 09/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 09/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 10/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 10/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 10/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 11/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 11/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 11/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 12/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 12/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 12/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 13/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 13/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 13/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 14/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 14/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 14/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 15/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 15/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 15/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 16/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 16/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 16/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 17/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 17/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 17/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 18/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 18/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 18/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 19/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 19/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 19/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 20/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 20/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 20/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 21/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 21/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 21/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 22/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 22/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 22/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Channel 23/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Channel 23/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Channel 23/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO Connected all trees
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO Connected all trees
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO Connected all trees
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO Connected all trees
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO Connected all trees
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO Connected all trees
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO Connected all trees
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO Connected all trees
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,0]<stdout>:compute-st-worker-41:16536:18217 [0] NCCL INFO comm 0x557e5604dfa0 rank 0 nranks 8 cudaDev 0 busId 101c0 - Init COMPLETE
[1,1]<stdout>:compute-st-worker-41:16537:18222 [1] NCCL INFO comm 0x55aa0fb7d5d0 rank 1 nranks 8 cudaDev 1 busId 101d0 - Init COMPLETE
[1,2]<stdout>:compute-st-worker-41:16538:18224 [2] NCCL INFO comm 0x562fc7784de0 rank 2 nranks 8 cudaDev 2 busId 201c0 - Init COMPLETE
[1,5]<stdout>:compute-st-worker-41:16541:18219 [5] NCCL INFO comm 0x55ab1eebe810 rank 5 nranks 8 cudaDev 5 busId 901d0 - Init COMPLETE
[1,3]<stdout>:compute-st-worker-41:16539:18218 [3] NCCL INFO comm 0x55c92561abc0 rank 3 nranks 8 cudaDev 3 busId 201d0 - Init COMPLETE
[1,4]<stdout>:compute-st-worker-41:16540:18221 [4] NCCL INFO comm 0x556dfe21de00 rank 4 nranks 8 cudaDev 4 busId 901c0 - Init COMPLETE
[1,7]<stdout>:compute-st-worker-41:16543:18223 [7] NCCL INFO comm 0x55a4eac0bd10 rank 7 nranks 8 cudaDev 7 busId a01d0 - Init COMPLETE
[1,6]<stdout>:compute-st-worker-41:16542:18220 [6] NCCL INFO comm 0x55876b82ae40 rank 6 nranks 8 cudaDev 6 busId a01c0 - Init COMPLETE
[1,0]<stdout>:[2023-03-23 19:28:33,944] [INFO] [utils.py:829:see_memory_usage] After creating fp16 partitions: 2
[1,0]<stdout>:[2023-03-23 19:28:33,945] [INFO] [utils.py:830:see_memory_usage] MA 1.98 GB         Max_MA 1.98 GB         CA 1.98 GB         Max_CA 4 GB 
[1,0]<stdout>:[2023-03-23 19:28:33,945] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 54.35 GB, percent = 4.8%
[1,0]<stdout>:[2023-03-23 19:28:34,030] [INFO] [utils.py:829:see_memory_usage] Before creating fp32 partitions
[1,0]<stdout>:[2023-03-23 19:28:34,031] [INFO] [utils.py:830:see_memory_usage] MA 1.98 GB         Max_MA 1.98 GB         CA 1.98 GB         Max_CA 2 GB 
[1,0]<stdout>:[2023-03-23 19:28:34,031] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 54.37 GB, percent = 4.8%
[1,0]<stdout>:[2023-03-23 19:28:34,122] [INFO] [utils.py:829:see_memory_usage] After creating fp32 partitions
[1,0]<stdout>:[2023-03-23 19:28:34,122] [INFO] [utils.py:830:see_memory_usage] MA 5.94 GB         Max_MA 7.92 GB         CA 7.93 GB         Max_CA 8 GB 
[1,0]<stdout>:[2023-03-23 19:28:34,123] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 54.37 GB, percent = 4.8%
[1,0]<stdout>:[2023-03-23 19:28:34,209] [INFO] [utils.py:829:see_memory_usage] Before initializing optimizer states
[1,0]<stdout>:[2023-03-23 19:28:34,209] [INFO] [utils.py:830:see_memory_usage] MA 5.94 GB         Max_MA 5.94 GB         CA 7.93 GB         Max_CA 8 GB 
[1,0]<stdout>:[2023-03-23 19:28:34,209] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 54.37 GB, percent = 4.8%
[1,0]<stdout>:[2023-03-23 19:28:34,521] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | init_optimizer_state: 29.58
[1,3]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,6]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,1]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,3]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,3]<stderr>:Loading extension module utils...
[1,1]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,1]<stderr>:Loading extension module utils...
[1,3]<stdout>:Time to load utils op: 0.0016849040985107422 seconds
[1,1]<stdout>:Time to load utils op: 0.0013649463653564453 seconds
[1,3]<stdout>: > number of parameters on model parallel rank 3: 0
[1,6]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,6]<stderr>:Loading extension module utils...
[1,1]<stdout>: > number of parameters on model parallel rank 1: 0
[1,6]<stdout>:Time to load utils op: 0.00167083740234375 seconds
[1,6]<stdout>: > number of parameters on model parallel rank 6: 0
[1,7]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,7]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,7]<stderr>:Loading extension module utils...
[1,7]<stdout>:Time to load utils op: 0.001493215560913086 seconds
[1,7]<stdout>: > number of parameters on model parallel rank 7: 0
[1,2]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,2]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,2]<stderr>:Loading extension module utils...
[1,2]<stdout>:Time to load utils op: 0.0015876293182373047 seconds
[1,2]<stdout>: > number of parameters on model parallel rank 2: 0
[1,5]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,4]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,4]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,4]<stderr>:Loading extension module utils...
[1,4]<stdout>:Time to load utils op: 0.0011484622955322266 seconds
[1,4]<stdout>: > number of parameters on model parallel rank 4: 0
[1,5]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,5]<stderr>:Loading extension module utils...
[1,5]<stdout>:Time to load utils op: 0.0017592906951904297 seconds
[1,5]<stdout>: > number of parameters on model parallel rank 5: 0
[1,0]<stdout>:[2023-03-23 19:28:34,606] [INFO] [utils.py:829:see_memory_usage] After initializing optimizer states
[1,0]<stdout>:[2023-03-23 19:28:34,607] [INFO] [utils.py:830:see_memory_usage] MA 13.87 GB         Max_MA 17.83 GB         CA 19.81 GB         Max_CA 20 GB 
[1,0]<stdout>:[2023-03-23 19:28:34,607] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.47 GB, percent = 4.1%
[1,0]<stdout>:[2023-03-23 19:28:34,608] [INFO] [stage3.py:376:_setup_for_real_optimizer] optimizer state initialized
[1,0]<stdout>:[2023-03-23 19:28:34,688] [INFO] [utils.py:829:see_memory_usage] After initializing ZeRO optimizer
[1,0]<stdout>:[2023-03-23 19:28:34,689] [INFO] [utils.py:830:see_memory_usage] MA 16.08 GB         Max_MA 16.38 GB         CA 19.81 GB         Max_CA 20 GB 
[1,0]<stdout>:[2023-03-23 19:28:34,689] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 46.47 GB, percent = 4.1%
[1,0]<stdout>:[2023-03-23 19:28:34,689] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[1,0]<stdout>:[2023-03-23 19:28:34,689] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[1,0]<stdout>:[2023-03-23 19:28:34,689] [INFO] [logging.py:77:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x7fd26c85b5e0>
[1,0]<stdout>:[2023-03-23 19:28:34,689] [INFO] [logging.py:77:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[[0.9, 0.95], [0.9, 0.95]]
[1,0]<stdout>:[2023-03-23 19:28:34,689] [INFO] [config.py:1018:print] DeepSpeedEngine configuration:
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   activation_checkpointing_config  {
[1,0]<stdout>:    "partition_activations": false, 
[1,0]<stdout>:    "contiguous_memory_optimization": false, 
[1,0]<stdout>:    "cpu_checkpointing": false, 
[1,0]<stdout>:    "number_checkpoints": null, 
[1,0]<stdout>:    "synchronize_checkpoint_boundary": false, 
[1,0]<stdout>:    "profile": false
[1,0]<stdout>:}
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   amp_enabled .................. False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   amp_params ................... False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   autotuning_config ............ {
[1,0]<stdout>:    "enabled": false, 
[1,0]<stdout>:    "start_step": null, 
[1,0]<stdout>:    "end_step": null, 
[1,0]<stdout>:    "metric_path": null, 
[1,0]<stdout>:    "arg_mappings": null, 
[1,0]<stdout>:    "metric": "throughput", 
[1,0]<stdout>:    "model_info": null, 
[1,0]<stdout>:    "results_dir": "autotuning_results", 
[1,0]<stdout>:    "exps_dir": "autotuning_exps", 
[1,0]<stdout>:    "overwrite": true, 
[1,0]<stdout>:    "fast": true, 
[1,0]<stdout>:    "start_profile_step": 3, 
[1,0]<stdout>:    "end_profile_step": 5, 
[1,0]<stdout>:    "tuner_type": "gridsearch", 
[1,0]<stdout>:    "tuner_early_stopping": 5, 
[1,0]<stdout>:    "tuner_num_trials": 50, 
[1,0]<stdout>:    "model_info_path": null, 
[1,0]<stdout>:    "mp_size": 1, 
[1,0]<stdout>:    "max_train_batch_size": null, 
[1,0]<stdout>:    "min_train_batch_size": 1, 
[1,0]<stdout>:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[1,0]<stdout>:    "min_train_micro_batch_size_per_gpu": 1, 
[1,0]<stdout>:    "num_tuning_micro_batch_sizes": 3
[1,0]<stdout>:}
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   bfloat16_enabled ............. False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   checkpoint_parallel_write_pipeline  False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   checkpoint_tag_validation_enabled  True
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   checkpoint_tag_validation_fail  False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd26c85b340>
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   communication_data_type ...... None
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   curriculum_enabled_legacy .... False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   curriculum_params_legacy ..... False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   data_efficiency_enabled ...... False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   dataloader_drop_last ......... False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   disable_allgather ............ False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   dump_state ................... False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   eigenvalue_enabled ........... False
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   eigenvalue_gas_boundary_resolution  1
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   eigenvalue_layer_name ........ bert.encoder.layer
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   eigenvalue_layer_num ......... 0
[1,0]<stdout>:[2023-03-23 19:28:34,690] [INFO] [config.py:1022:print]   eigenvalue_max_iter .......... 100
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   eigenvalue_stability ......... 1e-06
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   eigenvalue_tol ............... 0.01
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   eigenvalue_verbose ........... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   elasticity_enabled ........... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   flops_profiler_config ........ {
[1,0]<stdout>:    "enabled": false, 
[1,0]<stdout>:    "profile_step": 1, 
[1,0]<stdout>:    "module_depth": -1, 
[1,0]<stdout>:    "top_modules": 1, 
[1,0]<stdout>:    "detailed": true, 
[1,0]<stdout>:    "output_file": null
[1,0]<stdout>:}
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   fp16_auto_cast ............... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   fp16_enabled ................. True
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   fp16_master_weights_and_gradients  False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   global_rank .................. 0
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   grad_accum_dtype ............. None
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   gradient_accumulation_steps .. 1
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   gradient_clipping ............ 1.0
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   gradient_predivide_factor .... 1.0
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   initial_dynamic_scale ........ 4096
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   load_universal_checkpoint .... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   loss_scale ................... 0
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   memory_breakdown ............. False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   nebula_config ................ {
[1,0]<stdout>:    "enabled": false, 
[1,0]<stdout>:    "persistent_storage_path": null, 
[1,0]<stdout>:    "persistent_time_interval": 100, 
[1,0]<stdout>:    "num_of_version_in_retention": 2, 
[1,0]<stdout>:    "enable_nebula_load": true, 
[1,0]<stdout>:    "load_path": null
[1,0]<stdout>:}
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   optimizer_legacy_fusion ...... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   optimizer_name ............... adam
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   optimizer_params ............. {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-06}
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   pld_enabled .................. False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   pld_params ................... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   prescale_gradients ........... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   scheduler_name ............... None
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   scheduler_params ............. None
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   sparse_attention ............. None
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   sparse_gradients_enabled ..... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   steps_per_print .............. 10
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   train_batch_size ............. 16
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   train_micro_batch_size_per_gpu  16
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   use_node_local_storage ....... False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   wall_clock_breakdown ......... True
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   world_size ................... 1
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   zero_allow_untested_optimizer  False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=126000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=100000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   zero_enabled ................. True
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   zero_force_ds_cpu_optimizer .. True
[1,0]<stdout>:[2023-03-23 19:28:34,691] [INFO] [config.py:1022:print]   zero_optimization_stage ...... 3
[1,0]<stdout>:[2023-03-23 19:28:34,692] [INFO] [config.py:1007:print_user_config]   json = {
[1,0]<stdout>:    "train_batch_size": 16, 
[1,0]<stdout>:    "train_micro_batch_size_per_gpu": 16, 
[1,0]<stdout>:    "optimizer": {
[1,0]<stdout>:        "type": "Adam", 
[1,0]<stdout>:        "params": {
[1,0]<stdout>:            "lr": 0.0006, 
[1,0]<stdout>:            "betas": [0.9, 0.95], 
[1,0]<stdout>:            "eps": 1e-06
[1,0]<stdout>:        }
[1,0]<stdout>:    }, 
[1,0]<stdout>:    "fp16": {
[1,0]<stdout>:        "fp16": true, 
[1,0]<stdout>:        "enabled": true, 
[1,0]<stdout>:        "loss_scale": 0, 
[1,0]<stdout>:        "loss_scale_window": 1000, 
[1,0]<stdout>:        "initial_scale_power": 12, 
[1,0]<stdout>:        "hysteresis": 2, 
[1,0]<stdout>:        "min_loss_scale": 1
[1,0]<stdout>:    }, 
[1,0]<stdout>:    "gradient_clipping": 1.0, 
[1,0]<stdout>:    "zero_optimization": {
[1,0]<stdout>:        "stage": 3, 
[1,0]<stdout>:        "allgather_partitions": true, 
[1,0]<stdout>:        "overlap_comm": true, 
[1,0]<stdout>:        "reduce_scatter": true, 
[1,0]<stdout>:        "reduce_bucket_size": 1.260000e+08, 
[1,0]<stdout>:        "stage3_max_live_parameters": 1.000000e+08, 
[1,0]<stdout>:        "contiguous_gradients": true, 
[1,0]<stdout>:        "cpu_offload": false
[1,0]<stdout>:    }, 
[1,0]<stdout>:    "wall_clock_breakdown": true
[1,0]<stdout>:}
[1,0]<stderr>:Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[1,0]<stderr>:No modifications detected for re-loaded extension module utils, skipping build step...
[1,0]<stderr>:Loading extension module utils...
[1,0]<stdout>:Time to load utils op: 0.0014216899871826172 seconds
[1,0]<stdout>: > number of parameters on model parallel rank 0: 0
[1,0]<stdout>: > total params: 0
[1,0]<stdout>:> building train, validation, and test datasets ...
[1,0]<stdout>:    reading sizes...
[1,0]<stdout>:    reading pointers...
[1,0]<stdout>:    reading document index...
[1,0]<stdout>:    creating numpy buffer of mmap...
[1,0]<stdout>:    creating memory view of numpy buffer...
[1,0]<stdout>: > dataset split:
[1,0]<stdout>:    train:
[1,0]<stdout>:     document indices in [0, 1) total of 1 documents
[1,0]<stdout>:    validation:
[1,0]<stdout>:     document indices in [1, 1) total of 0 documents
[1,0]<stdout>:    test:[1,0]<stdout>:
[1,0]<stdout>:     document indices in [1, 1) total of 0 documents
[1,0]<stdout>: > loading doc-idx mapping from /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document_train_indexmap_48ns_4096sl_1234s_doc_idx.npy
[1,0]<stdout>: > loading sample-idx mapping from /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document_train_indexmap_48ns_4096sl_1234s_sample_idx.npy
[1,0]<stdout>: > loading shuffle-idx mapping from /fsx/sliuxl/EleutherAI/gpt-neox/data-from-leecheng/enwik8/enwik8_text_document_train_indexmap_48ns_4096sl_1234s_shuffle_idx.npy
[1,0]<stdout>:    loaded indexed file in 0.012 seconds
[1,0]<stdout>:    total number of samples: 7084
[1,0]<stdout>:    total number of epochs: 1
[1,0]<stdout>:WARNING: shuffle index length (7082) is not equal to sample index length (7083)
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Using network AWS Libfabric
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Using network AWS Libfabric
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Using network AWS Libfabric
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Using network AWS Libfabric
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Using network AWS Libfabric
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Using network AWS Libfabric
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Using network AWS Libfabric
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Using network AWS Libfabric
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 00/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 00/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 01/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 02/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 03/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 03/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 04/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 04/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 05/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 06/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 07/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 07/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 08/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 08/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 08/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 08/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 09/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 09/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 09/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 08/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 09/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 10/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 10/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 10/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 09/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 10/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 11/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 11/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 11/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 10/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 11/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 12/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 12/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 12/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 11/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 12/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 13/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 13/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 13/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 13/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 12/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 14/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 14/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 14/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 13/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 14/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 15/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 15/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 15/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 14/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 15/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 16/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 16/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 16/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 15/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 16/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 17/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 17/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 17/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 16/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 17/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 18/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 18/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 18/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 17/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 18/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 19/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 19/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 19/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 18/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 19/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 20/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 20/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 20/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 19/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 20/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 21/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 21/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 21/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 20/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 21/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 22/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 22/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 22/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 2[201c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 21/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 3[201d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 22/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 4[901c0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Channel 23/0 : 0[101c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 23/0 : 5[901d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 23/0 : 4[901c0] -> 5[901d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 22/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 23/0 : 7[a01d0] -> 0[101c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 23/0 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Connected all rings
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Connected all rings
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Connected all rings
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Connected all rings
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Connected all rings
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Connected all rings
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Connected all rings
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 00/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Connected all rings
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 01/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 02/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 03/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 04/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 05/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 06/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 07/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 08/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 09/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 10/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 11/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 12/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 13/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 14/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 15/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 16/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 17/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 18/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 19/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 20/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 21/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 00/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 00/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 00/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 22/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 00/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 01/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 01/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 01/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Channel 23/0 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 01/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 00/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 02/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 02/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 02/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 02/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 00/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 01/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 03/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 03/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 03/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 03/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 01/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 02/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 04/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 04/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 04/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 04/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 02/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 03/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 05/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 05/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 05/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 05/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 03/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 06/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 04/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 06/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 06/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 06/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 04/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 07/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 05/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 07/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 07/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 07/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 05/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 08/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 08/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 08/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 06/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 08/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 06/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 09/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 09/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 07/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 09/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 09/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 07/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 10/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 10/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 08/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 10/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 10/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 08/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 11/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 11/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 09/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 11/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 11/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 09/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 12/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 12/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 10/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 12/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 12/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 10/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 13/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 13/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 13/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 11/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 13/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 11/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 14/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 14/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 14/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 12/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 14/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 12/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 15/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 15/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 15/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 13/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 15/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 13/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 16/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 16/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 16/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 14/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 16/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 14/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 17/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 17/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 17/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 15/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 17/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 15/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 18/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 18/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 18/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 16/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 18/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 16/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 19/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 19/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 19/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 17/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 19/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 17/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 20/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 20/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 20/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 18/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 20/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 18/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 21/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 21/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 21/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 19/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 21/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 19/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 22/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 22/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 22/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 20/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 22/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 20/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Channel 23/0 : 2[201c0] -> 1[101d0] via P2P/IPC/read
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Channel 23/0 : 3[201d0] -> 2[201c0] via P2P/IPC/read
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Channel 23/0 : 1[101d0] -> 0[101c0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 21/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Channel 23/0 : 4[901c0] -> 3[201d0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 21/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 22/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 22/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Channel 23/0 : 5[901d0] -> 4[901c0] via P2P/IPC/read
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Channel 23/0 : 6[a01c0] -> 5[901d0] via P2P/IPC/read
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO Connected all trees
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO Connected all trees
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO Connected all trees
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO Connected all trees
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO Connected all trees
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO Connected all trees
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO Connected all trees
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO Connected all trees
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
[1,1]<stdout>:compute-st-worker-41:16537:18246 [1] NCCL INFO comm 0x55aa0fc2d710 rank 1 nranks 8 cudaDev 1 busId 101d0 - Init COMPLETE
[1,5]<stdout>:compute-st-worker-41:16541:18248 [5] NCCL INFO comm 0x55ab1ef0a8e0 rank 5 nranks 8 cudaDev 5 busId 901d0 - Init COMPLETE
[1,4]<stdout>:compute-st-worker-41:16540:18243 [4] NCCL INFO comm 0x556df90c46d0 rank 4 nranks 8 cudaDev 4 busId 901c0 - Init COMPLETE
[1,3]<stdout>:compute-st-worker-41:16539:18244 [3] NCCL INFO comm 0x55c92570ae20 rank 3 nranks 8 cudaDev 3 busId 201d0 - Init COMPLETE
[1,2]<stdout>:compute-st-worker-41:16538:18247 [2] NCCL INFO comm 0x562fc7740a80 rank 2 nranks 8 cudaDev 2 busId 201c0 - Init COMPLETE
[1,0]<stdout>:compute-st-worker-41:16536:18242 [0] NCCL INFO comm 0x557e560b93a0 rank 0 nranks 8 cudaDev 0 busId 101c0 - Init COMPLETE
[1,0]<stdout>:setting training data start iteration to 0
[1,7]<stdout>:compute-st-worker-41:16543:18245 [7] NCCL INFO comm 0x55a4eac5d950 rank 7 nranks 8 cudaDev 7 busId a01d0 - Init COMPLETE
[1,6]<stdout>:compute-st-worker-41:16542:18249 [6] NCCL INFO comm 0x55876b7d1260 rank 6 nranks 8 cudaDev 6 busId a01c0 - Init COMPLETE
[1,0]<stdout>:[2023-03-23 19:28:36.990 compute-st-worker-41:16536 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,0]<stdout>:done with setups ...
[1,0]<stdout>:time (ms) | model and optimizer: 8460.74 | train/valid/test data iterators: 2361.20
[1,0]<stdout>:training ...
[1,0]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,0]<stderr>:  warnings.warn(
[1,3]<stdout>:[2023-03-23 19:28:37.309 compute-st-worker-41:16539 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,1]<stdout>:[2023-03-23 19:28:37.310 compute-st-worker-41:16537 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,4]<stdout>:[2023-03-23 19:28:37.312 compute-st-worker-41:16540 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,5]<stdout>:[2023-03-23 19:28:37.313 compute-st-worker-41:16541 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,6]<stdout>:[2023-03-23 19:28:37.313 compute-st-worker-41:16542 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,2]<stdout>:[2023-03-23 19:28:37.313 compute-st-worker-41:16538 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,7]<stdout>:[2023-03-23 19:28:37.313 compute-st-worker-41:16543 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None
[1,3]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,3]<stderr>:  warnings.warn(
[1,1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,1]<stderr>:  warnings.warn(
[1,4]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,4]<stderr>:  warnings.warn(
[1,5]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,5]<stderr>:  warnings.warn(
[1,6]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,6]<stderr>:  warnings.warn(
[1,2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,2]<stderr>:  warnings.warn(
[1,7]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
[1,7]<stderr>:  warnings.warn(
[1,0]<stdout>:[2023-03-23 19:28:37,530] [INFO] [checkpointing.py:553:forward] Activation Checkpointing Information
[1,0]<stdout>:[2023-03-23 19:28:37,530] [INFO] [checkpointing.py:554:forward] ----Partition Activations True, CPU CHECKPOINTING False
[1,0]<stdout>:[2023-03-23 19:28:37,530] [INFO] [checkpointing.py:557:forward] ----contiguous Memory Checkpointing False with 4 total layers
[1,0]<stdout>:[2023-03-23 19:28:37,531] [INFO] [checkpointing.py:560:forward] ----Synchronization True
[1,0]<stdout>:[2023-03-23 19:28:37,531] [INFO] [checkpointing.py:561:forward] ----Profiling time in checkpointing False
[1,5]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,5]<stderr>:  warnings.warn(
[1,7]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,7]<stderr>:  warnings.warn(
[1,4]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,4]<stderr>:  warnings.warn(
[1,3]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,3]<stderr>:  warnings.warn(
[1,1]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,1]<stderr>:  warnings.warn(
[1,6]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,6]<stderr>:  warnings.warn(
[1,2]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,2]<stderr>:  warnings.warn(
[1,0]<stderr>:/opt/conda/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py:2849: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.
[1,0]<stderr>:  warnings.warn(
[1,0]<stdout>:[2023-03-23 19:28:42,461] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 367.25
[1,0]<stdout>:[2023-03-23 19:28:42,461] [WARNING] [stage3.py:1948:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[1,0]<stdout>:[2023-03-23 19:28:42,462] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 2558.68 | backward_microstep: 2219.19 | backward_inner_microstep: 2217.55 | backward_allreduce_microstep: 1.53 | step_microstep: 388.75
[1,0]<stdout>:[2023-03-23 19:28:42,462] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | forward: 2558.67 | backward: 2219.19 | backward_inner: 2217.56 | backward_allreduce: 1.54 | step: 388.74
[1,0]<stdout>:[2023-03-23 19:28:45,771] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 45.68
[1,0]<stdout>:[2023-03-23 19:28:45,772] [WARNING] [stage3.py:1948:step] 2 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[1,0]<stdout>:[2023-03-23 19:28:45,772] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 802.25 | backward_microstep: 2376.12 | backward_inner_microstep: 2373.48 | backward_allreduce_microstep: 2.58 | step_microstep: 65.65
[1,0]<stdout>:[2023-03-23 19:28:45,772] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | forward: 802.24 | backward: 2376.12 | backward_inner: 2373.48 | backward_allreduce: 2.59 | step: 65.65
[1,0]<stdout>:[2023-03-23 19:28:49,216] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | optimizer_step: 45.59
[1,0]<stdout>:[2023-03-23 19:28:49,216] [WARNING] [stage3.py:1948:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time
[1,0]<stdout>:[2023-03-23 19:28:49,216] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | forward_microstep: 764.78 | backward_microstep: 2598.06 | backward_inner_microstep: 2595.41 | backward_allreduce_microstep: 2.59 | step_microstep: 66.06
[1,0]<stdout>:[2023-03-23 19:28:49,216] [INFO] [logging.py:77:log_dist] [Rank 0] rank=0 time (ms) | forward: 764.77 | backward: 2598.06 | backward_inner: 2595.41 | backward_allreduce: 2.60 | step: 66.06
[1,3]<stdout>:compute-st-worker-41:16539:18231 [3] NCCL INFO [Service thread] Connection closed by localRank 3
[1,6]<stdout>:compute-st-worker-41:16542:18228 [6] NCCL INFO [Service thread] Connection closed by localRank 6
[1,1]<stdout>:compute-st-worker-41:16537:18225 [1] NCCL INFO [Service thread] Connection closed by localRank 1
[1,2]<stdout>:compute-st-worker-41:16538:18229 [2] NCCL INFO [Service thread] Connection closed by localRank 2
[1,5]<stdout>:compute-st-worker-41:16541:18226 [5] NCCL INFO [Service thread] Connection closed by localRank 5
[1,7]<stdout>:compute-st-worker-41:16543:18232 [7] NCCL INFO [Service thread] Connection closed by localRank 7
[1,0]<stdout>:compute-st-worker-41:16536:18227 [0] NCCL INFO [Service thread] Connection closed by localRank 0
[1,4]<stdout>:compute-st-worker-41:16540:18230 [4] NCCL INFO [Service thread] Connection closed by localRank 4
[1,0]<stdout>:compute-st-worker-41:16536:16536 [0] NCCL INFO comm 0x557e5604dfa0 rank 0 nranks 8 cudaDev 0 busId 101c0 - Abort COMPLETE
[1,6]<stdout>:compute-st-worker-41:16542:16542 [6] NCCL INFO comm 0x55876b82ae40 rank 6 nranks 8 cudaDev 6 busId a01c0 - Abort COMPLETE
[1,1]<stdout>:compute-st-worker-41:16537:16537 [1] NCCL INFO comm 0x55aa0fb7d5d0 rank 1 nranks 8 cudaDev 1 busId 101d0 - Abort COMPLETE
[1,3]<stdout>:compute-st-worker-41:16539:16539 [3] NCCL INFO comm 0x55c92561abc0 rank 3 nranks 8 cudaDev 3 busId 201d0 - Abort COMPLETE
[1,7]<stdout>:compute-st-worker-41:16543:16543 [7] NCCL INFO comm 0x55a4eac0bd10 rank 7 nranks 8 cudaDev 7 busId a01d0 - Abort COMPLETE
[1,2]<stdout>:compute-st-worker-41:16538:16538 [2] NCCL INFO comm 0x562fc7784de0 rank 2 nranks 8 cudaDev 2 busId 201c0 - Abort COMPLETE
[1,5]<stdout>:compute-st-worker-41:16541:16541 [5] NCCL INFO comm 0x55ab1eebe810 rank 5 nranks 8 cudaDev 5 busId 901d0 - Abort COMPLETE
[1,4]<stdout>:compute-st-worker-41:16540:16540 [4] NCCL INFO comm 0x556dfe21de00 rank 4 nranks 8 cudaDev 4 busId 901c0 - Abort COMPLETE
+ rm /fsx/sliuxl/EleutherAI/gpt-neox/hostfile_mpi
+ scancel -n hb_29740
+ scancel -n train_29740

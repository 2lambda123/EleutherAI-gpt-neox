# Model


## v1
training on the dataset below with llama-cn tokenizer: 
  - PileCC 224G （来自pile）
  - Github 99G （来自pile）
  - Gutenberg  11G + books3  102G  = 113G (来自pile)
  - Arxiv 58G （来自pile）
  - Stack Exchange 34G （来自pile）
  - Openwebtext2 67G （来自pile）
  - Wikipedia-multilingual 74G （自采）

totally 245.2B (245214265521) tokens


## v2
training on the dataset below with gpt2 tokenizer: 
  - PileCC 224G （来自pile）
  - Github 99G （来自pile）
  - Gutenberg  11G + books3  102G  = 113G (来自pile)
  - Arxiv 58G （来自pile）
  - Stack Exchange 34G （来自pile）
  - Openwebtext2 67G （来自pile）
  - Wikipedia-multilingual 74G （自采）

totally 259.6B (259663447604) tokens